#!/usr/bin/env python3
"""
Stage 5: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫
–ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ N/D, —á–∞—Å—Ç–æ—Ç—ã –∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
"""

import json
import logging
import pandas as pd
from pathlib import Path
from collections import Counter, defaultdict
from typing import List, Dict, Any, Tuple

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from config import settings
from models.validation import DeliveryDetection, ClustersData, AggregateResults
from prompts import STAGE_CONFIG, DELIVERY_KEYWORDS, PLATFORM_KEYWORDS

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def load_jsonl(file_path: str) -> List[Dict[str, Any]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ JSONL —Ñ–∞–π–ª–∞"""
    data = []
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data.append(json.loads(line.strip()))
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å—Ç—Ä–æ–∫–∏: {e}")
                continue
    return data


def unique_count_and_ids(records_for_cluster):
    """–ü–æ–¥—Å—á–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –∏ –∏—Ö ID"""
    ids = {r["dialog_id"] for r in records_for_cluster}
    return len(ids), sorted(ids)


def pct_of_D(x, D):
    """–ü—Ä–æ—Ü–µ–Ω—Ç –æ—Ç D"""
    return round(100.0 * x / D, 1) if D else 0.0


def split_ideas_by_role(records):
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∏–¥–µ–π –ø–æ —Ä–æ–ª—è–º: –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–µ vs –æ–ø–µ—Ä–∞—Ç–æ—Ä—Å–∫–∏–µ"""
    client_side, operator_side = [], []
    for r in records:
        ci = [i for i in r.get("ideas", []) if i.get("source_role") == "client"]
        oi = [i for i in r.get("ideas", []) if i.get("source_role") == "operator"]
        if ci:
            rr = dict(r)
            rr["ideas"] = ci
            client_side.append(rr)
        if oi:
            rr = dict(r)
            rr["ideas"] = oi
            operator_side.append(rr)
    return client_side, operator_side


def promote_questions_to_barriers(record):
    """–≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã/–Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏–µ ‚Üí –≤ –ë–∞—Ä—å–µ—Ä—ã"""
    new_barriers = record.get("barriers", [])[:]
    for i in record.get("ideas", []):
        txt = (i.get("text") or "").lower()
        if any(k in txt for k in ["–∫–∞–∫ ", "–ø–æ—á–µ–º—É", "–Ω–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–æ", "–≥–¥–µ –∫–Ω–æ–ø–∫–∞", "–Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç"]):
            new_barriers.append("–Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏–µ/–Ω–µ–≤–µ—Ä–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–æ—Å—Ç–∞–≤–∫–∏")
    record["barriers"] = list({b.strip() for b in new_barriers})
    return record


def split_sections(records, require_client_role_for_ideas=True):
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –∑–∞–ø–∏—Å–µ–π –ø–æ —Å–µ–∫—Ü–∏—è–º"""
    barriers, ideas_client, signals, operator_recos = [], [], [], []
    for r in records:
        src = r.get("source_role", "client")
        labs = r.get("labels", {})
        if labs.get("barrier"):
            barriers.append(r)
        elif src == "client" and labs.get("idea"):
            ideas_client.append(r)
        elif labs.get("signal"):
            signals.append(r)
        elif src == "operator":
            operator_recos.append(r)
    if not require_client_role_for_ideas:
        # –º–æ–∂–Ω–æ –æ–±—ä–µ–¥–∏–Ω–∏—Ç—å –ø–æ –∂–µ–ª–∞–Ω–∏—é
        pass
    return barriers, ideas_client, signals, operator_recos


def cluster_mentions(cluster_records):
    """–ü–æ–¥—Å—á–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –ø–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–º dialog_id"""
    return len({r["dialog_id"] for r in cluster_records})


def classify_cluster(records_for_cluster):
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ –¥–æ–ª–µ –¥–æ—Å—Ç–∞–≤–æ—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
    total = len({r["dialog_id"] for r in records_for_cluster})
    delivery_ids = {r["dialog_id"] for r in records_for_cluster if r.get("delivery_discussed")}
    share = len(delivery_ids) / total if total else 0.0
    if share >= 0.6:  # –ø–æ—Ä–æ–≥ –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –∫–æ–Ω—Ñ–∏–≥
        return "delivery"
    return "platform_signals"


def classify_cluster_by_dialogs(cluster: Dict[str, Any]) -> str:
    """–ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∫–ª–∞—Å—Ç–µ—Ä–∞ –ø–æ –¥–æ–ª–µ –¥–æ—Å—Ç–∞–≤–æ—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
    dialog_ids = cluster.get("dialog_ids", [])
    if not dialog_ids:
        return "platform_signals"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –¥–æ—Å—Ç–∞–≤–∫–µ –¥–ª—è —ç—Ç–∏—Ö –¥–∏–∞–ª–æ–≥–æ–≤
    detections_data = load_jsonl("artifacts/stage1_delivery.jsonl")
    delivery_dialog_ids = {d["dialog_id"] for d in detections_data if d.get("delivery_discussed")}
    
    # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –¥–æ–ª—é –¥–æ—Å—Ç–∞–≤–æ—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –≤ –∫–ª–∞—Å—Ç–µ—Ä–µ
    delivery_count = sum(1 for dialog_id in dialog_ids if dialog_id in delivery_dialog_ids)
    total_count = len(dialog_ids)
    share = delivery_count / total_count if total_count else 0.0
    
    if share >= 0.6:  # –ø–æ—Ä–æ–≥ –º–æ–∂–Ω–æ –≤—ã–Ω–µ—Å—Ç–∏ –≤ –∫–æ–Ω—Ñ–∏–≥
        return "delivery"
    return "platform_signals"


def separate_platform_signals(clusters: List[Dict[str, Any]]) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞ –¥–æ—Å—Ç–∞–≤–∫—É –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É –ø–æ –¥–æ–ª–µ –¥–æ—Å—Ç–∞–≤–æ—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
    delivery_signals = []
    platform_signals = []
    
    for cluster in clusters:
        # –ö–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä—É–µ–º –∫–ª–∞—Å—Ç–µ—Ä –ø–æ –¥–æ–ª–µ –¥–æ—Å—Ç–∞–≤–æ—á–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
        cluster_type = classify_cluster_by_dialogs(cluster)
        
        if cluster_type == "delivery":
            delivery_signals.append(cluster)
        else:
            platform_signals.append(cluster)
    
    return delivery_signals, platform_signals


def load_delivery_detections() -> List[DeliveryDetection]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–æ—Å—Ç–∞–≤–∫–∏"""
    delivery_file = Path("artifacts/stage1_delivery.jsonl")
    
    if not delivery_file.exists():
        logger.error(f"‚ùå –§–∞–π–ª {delivery_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 1 —Å–Ω–∞—á–∞–ª–∞.")
        return []
    
    detections = []
    
    with open(delivery_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                detection = DeliveryDetection(**data)
                detections.append(detection)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏: {e}")
                continue
    
    return detections


def load_clusters() -> ClustersData:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏"""
    clusters_file = Path("artifacts/stage4_clusters.json")
    
    if not clusters_file.exists():
        logger.error(f"‚ùå –§–∞–π–ª {clusters_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 4 —Å–Ω–∞—á–∞–ª–∞.")
        return ClustersData()
    
    with open(clusters_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return ClustersData(**data)


def calculate_quality_metrics(detections: List[DeliveryDetection], clusters: ClustersData) -> Dict[str, int]:
    """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    # N = –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤
    total_dialogs = len(detections)
    
    # D = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π
    delivery_dialogs = sum(1 for d in detections if d.delivery_discussed)
    
    # X = –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π, –Ω–æ –±–µ–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π
    delivery_dialog_ids = {d.dialog_id for d in detections if d.delivery_discussed}
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ ID –¥–∏–∞–ª–æ–≥–æ–≤ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏
    entity_dialog_ids = set()
    for cluster in clusters.barriers + clusters.ideas + clusters.signals:
        entity_dialog_ids.update(cluster.dialog_ids)
    
    # X = –¥–∏–∞–ª–æ–≥–∏ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π, –Ω–æ –±–µ–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π
    delivery_without_entities = len(delivery_dialog_ids - entity_dialog_ids)
    
    # Y = –¥–∏–∞–ª–æ–≥–∏ –±–µ–∑ –¥–æ—Å—Ç–∞–≤–∫–∏, –Ω–æ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏
    non_delivery_dialog_ids = {d.dialog_id for d in detections if not d.delivery_discussed}
    non_delivery_with_entities = len(non_delivery_dialog_ids & entity_dialog_ids)
    
    return {
        "total_dialogs": total_dialogs,
        "delivery_dialogs": delivery_dialogs,
        "delivery_without_entities": delivery_without_entities,
        "non_delivery_with_entities": non_delivery_with_entities
    }


def per_dialog_counts(records_for_cluster):
    """–ü–æ–¥—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –ø–µ—Ä-–¥–∏–∞–ª–æ–≥ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    unique_ids = sorted({r["dialog_id"] for r in records_for_cluster})
    
    # –í–∞—Ä–∏–∞–Ω—Ç—ã = –ø–æ –¥–∏–∞–ª–æ–≥–∞–º
    variant_to_ids = defaultdict(set)
    for r in records_for_cluster:
        seen = set()
        for v in (r.get("variants_in_this_cluster") or r.get("barriers") or []):
            nv = v.strip().lower()
            if nv in seen: 
                continue
            variant_to_ids[nv].add(r["dialog_id"])
            seen.add(nv)
    variants = [{"text": v, "count_abs": len(ids), "dialog_ids": sorted(ids)} for v, ids in variant_to_ids.items()]

    # –°—Ä–µ–∑—ã
    regions = defaultdict(set)
    sentiments = defaultdict(set)
    delivery_types = defaultdict(set)
    segments = defaultdict(set)
    categories = defaultdict(set)
    
    for r in records_for_cluster:
        did = r["dialog_id"]
        if r.get("region"): 
            regions[r["region"]].add(did)
        if r.get("segment"): 
            segments[r["segment"]].add(did)
        if r.get("product_category"): 
            categories[r["product_category"]].add(did)
        for t in r.get("delivery_types", []): 
            delivery_types[t].add(did)
        if r.get("sentiment"): 
            sentiments[r["sentiment"]].add(did)

    # –°–æ–±–∏—Ä–∞–µ–º —Ü–∏—Ç–∞—Ç—ã –∏–∑ –≤—Å–µ—Ö –∑–∞–ø–∏—Å–µ–π –∫–ª–∞—Å—Ç–µ—Ä–∞
    quotes = []
    low_evidence_count = 0
    for r in records_for_cluster:
        if r.get("citations"):
            for citation in r["citations"]:
                if isinstance(citation, dict) and citation.get("quote"):
                    quotes.append({
                        "quote": citation["quote"],
                        "dialog_id": r["dialog_id"],
                        "speaker": citation.get("speaker", "–ö–ª–∏–µ–Ω—Ç")
                    })
        if r.get("extras", {}).get("low_evidence"):
            low_evidence_count += 1
    
    return {
        "mentions_abs": len(unique_ids),
        "dialog_ids": unique_ids,
        "variants": variants,
        "quotes": quotes,
        "low_evidence_share": low_evidence_count / len(unique_ids) if unique_ids else 0.0,
        "slices": {
            "regions": {k: len(v) for k, v in regions.items()},
            "segments": {k: len(v) for k, v in segments.items()},
            "product_categories": {k: len(v) for k, v in categories.items()},
            "delivery_types": {k: len(v) for k, v in delivery_types.items()},
            "sentiment": {k: len(v) for k, v in sentiments.items()},
        }
    }


def calculate_cluster_metrics(clusters: List[Any], D: int, extractions: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """–†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å —É—á–µ—Ç–æ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
    cluster_metrics = []
    
    for i, cluster in enumerate(clusters):
        # –ü–æ–ª—É—á–∞–µ–º –∑–∞–ø–∏—Å–∏ –¥–ª—è —ç—Ç–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
        cluster_dialog_ids = sorted(set(cluster.dialog_ids))
        records_for_cluster = [ext for ext in extractions if ext.get("dialog_id") in cluster_dialog_ids]
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é per_dialog_counts
        cluster_data = per_dialog_counts(records_for_cluster)
        
        # –û–±–Ω–æ–≤–ª—è–µ–º mentions_abs –∏ dialog_ids
        mentions_abs = cluster_data["mentions_abs"]
        cluster_dialog_ids = cluster_data["dialog_ids"]
        
        # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Å –ø–æ–¥—Å—á–µ—Ç–æ–º –ø–æ –¥–∏–∞–ª–æ–≥–∞–º
        variants_metrics = []
        for variant in cluster_data["variants"]:
            variants_metrics.append({
                "text": variant["text"],
                "count_abs": variant["count_abs"],
                "count_pct_of_D": pct_of_D(variant["count_abs"], D),
                "dialog_ids": variant["dialog_ids"]
            })
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ä–µ–∑—ã –∏–∑ per_dialog_counts
        slices_metrics = cluster_data["slices"]
        
        cluster_metric = {
            "cluster_id": i + 1,
            "name": cluster.name,
            "mentions_abs": mentions_abs,
            "mentions_pct_of_D": pct_of_D(mentions_abs, D),
            "dialog_ids": cluster_dialog_ids,
            "variants": variants_metrics,
            "quotes": cluster_data.get("quotes", []),
            "low_evidence_share": cluster_data.get("low_evidence_share", 0.0),
            "slices": slices_metrics
        }
        
        cluster_metrics.append(cluster_metric)
    
    return cluster_metrics


def save_csv_reports(barrier_metrics: List[Dict], idea_metrics: List[Dict], signal_metrics: List[Dict]):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ CSV –æ—Ç—á–µ—Ç–æ–≤"""
    artifacts_dir = Path("artifacts")
    artifacts_dir.mkdir(exist_ok=True)
    
    # –ë–∞—Ä—å–µ—Ä—ã
    if barrier_metrics:
        barrier_df = pd.DataFrame(barrier_metrics)
        barrier_df.to_csv(artifacts_dir / "barriers.csv", index=False, encoding='utf-8')
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –æ—Ç—á–µ—Ç barriers.csv —Å {len(barrier_metrics)} –∑–∞–ø–∏—Å—è–º–∏")
    
    # –ò–¥–µ–∏
    if idea_metrics:
        idea_df = pd.DataFrame(idea_metrics)
        idea_df.to_csv(artifacts_dir / "ideas.csv", index=False, encoding='utf-8')
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –æ—Ç—á–µ—Ç ideas.csv —Å {len(idea_metrics)} –∑–∞–ø–∏—Å—è–º–∏")
    
    # –°–∏–≥–Ω–∞–ª—ã
    if signal_metrics:
        signal_df = pd.DataFrame(signal_metrics)
        signal_df.to_csv(artifacts_dir / "signals.csv", index=False, encoding='utf-8')
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω –æ—Ç—á–µ—Ç signals.csv —Å {len(signal_metrics)} –∑–∞–ø–∏—Å—è–º–∏")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Stage 5"""
    logger.info("üöÄ Stage 5: –ê–≥—Ä–µ–≥–∞—Ü–∏—è –º–µ—Ç—Ä–∏–∫")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ artifacts
    artifacts_dir = Path("artifacts")
    artifacts_dir.mkdir(exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–æ—Å—Ç–∞–≤–∫–∏...")
    detections = load_delivery_detections()
    
    logger.info("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π...")
    extractions = load_jsonl("artifacts/stage2_extracted.jsonl")
    
    logger.info("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏...")
    clusters = load_clusters()
    
    if not detections:
        logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–µ—Ç–µ–∫—Ü–∏–∏ –¥–æ—Å—Ç–∞–≤–∫–∏")
        return
    
    if not clusters.barriers and not clusters.ideas and not clusters.signals:
        logger.error("‚ùå –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        return
    
    # –†–∞—Å—á–µ—Ç –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ N –∏ D
    N = len(detections)  # total_calls_from_excel
    detections_data = load_jsonl("artifacts/stage1_delivery.jsonl")
    D = sum(1 for d in detections_data if d.get("delivery_discussed") is True)
    
    logger.info(f"üìä N (–≤—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤): {N}")
    logger.info(f"üìä D (—Å –¥–æ—Å—Ç–∞–≤–∫–æ–π): {D}")
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞
    quality_metrics = calculate_quality_metrics(detections, clusters)
    
    # –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    logger.info("üîÑ –†–∞—Å—á–µ—Ç –º–µ—Ç—Ä–∏–∫ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤...")
    barrier_metrics = calculate_cluster_metrics(clusters.barriers, D, extractions)
    idea_metrics = calculate_cluster_metrics(clusters.ideas, D, extractions)
    signal_metrics = calculate_cluster_metrics(clusters.signals, D, extractions)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞ –¥–æ—Å—Ç–∞–≤–∫—É –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É
    logger.info("üîÑ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Å–∏–≥–Ω–∞–ª–æ–≤ –Ω–∞ –¥–æ—Å—Ç–∞–≤–∫—É –∏ –ø–ª–∞—Ç—Ñ–æ—Ä–º—É...")
    delivery_signals, platform_signals = separate_platform_signals(signal_metrics)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    aggregate_result = {
        "barriers": barrier_metrics,
        "ideas": idea_metrics,
        "signals": delivery_signals,
        "signals_platform": platform_signals,
        "meta": {"N": N, "D": D}
    }
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_file = artifacts_dir / "aggregate_results.json"
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(aggregate_result, f, ensure_ascii=False, indent=2)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ CSV –æ—Ç—á–µ—Ç–æ–≤
    save_csv_reports(barrier_metrics, idea_metrics, signal_metrics)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏:")
    logger.info(f"  –í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤ (N): {N}")
    logger.info(f"  –° –¥–æ—Å—Ç–∞–≤–∫–æ–π (D): {D} ({pct_of_D(D, N)}%)")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –±–∞—Ä—å–µ—Ä–æ–≤: {len(barrier_metrics)}")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–¥–µ–π: {len(idea_metrics)}")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–æ—Å—Ç–∞–≤–∫–∏: {len(delivery_signals)}")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã: {len(platform_signals)}")
    
    # –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    logger.info("üîç –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –ø–∞–π–ø–ª–∞–π–Ω–∞:")
    logger.info(f"  –° –¥–æ—Å—Ç–∞–≤–∫–æ–π, –Ω–æ –±–µ–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π (X): {quality_metrics['delivery_without_entities']}")
    logger.info(f"  –ë–µ–∑ –¥–æ—Å—Ç–∞–≤–∫–∏, –Ω–æ —Å —Å—É—â–Ω–æ—Å—Ç—è–º–∏ (Y): {quality_metrics['non_delivery_with_entities']}")
    
    # –¢–æ–ø-–∫–ª–∞—Å—Ç–µ—Ä—ã
    if barrier_metrics:
        logger.info("üîç –¢–æ–ø-3 –∫–ª–∞—Å—Ç–µ—Ä–∞ –±–∞—Ä—å–µ—Ä–æ–≤:")
        for cluster in barrier_metrics[:3]:
            logger.info(f"  {cluster['cluster_id']}. {cluster['name']} - {cluster['mentions_abs']} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π ({cluster['mentions_pct_of_D']}%)")
    
    if idea_metrics:
        logger.info("üîç –¢–æ–ø-3 –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏–¥–µ–π:")
        for cluster in idea_metrics[:3]:
            logger.info(f"  {cluster['cluster_id']}. {cluster['name']} - {cluster['mentions_abs']} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π ({cluster['mentions_pct_of_D']}%)")
    
    if platform_signals:
        logger.info("üîç –¢–æ–ø-3 –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã:")
        for cluster in platform_signals[:3]:
            logger.info(f"  {cluster['cluster_id']}. {cluster['name']} - {cluster['mentions_abs']} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π ({cluster['mentions_pct_of_D']}%)")
    
    logger.info("‚úÖ Stage 5 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    main()
