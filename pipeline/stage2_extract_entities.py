#!/usr/bin/env python3
"""
Stage 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
–ò–∑–≤–ª–µ–∫–∞–µ—Ç –±–∞—Ä—å–µ—Ä—ã, –∏–¥–µ–∏, —Å–∏–≥–Ω–∞–ª—ã –∏ –¥—Ä—É–≥–∏–µ –ø–æ–ª—è –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π
"""

import json
import logging
import pandas as pd
import re
from pathlib import Path
from tqdm import tqdm
from tenacity import retry, stop_after_attempt, wait_exponential
from typing import List, Dict, Any, Set

import sys
sys.path.append(str(Path(__file__).parent.parent))

import openai
from config import settings
from models.validation import DeliveryDetection, DialogExtraction, Citation
from prompts import ENTITY_EXTRACTION_PROMPT, STAGE_CONFIG, DELIVERY_KEYWORDS

# –ñ–µ—Å—Ç–∫–∏–π —Å–ª–æ–≤–∞—Ä—å —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–µ–π
ALLOWED_SENTIMENTS = {"—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ", "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ", "—Å–æ–º–Ω–µ–Ω–∏–µ", "–ø–æ–∑–∏—Ç–∏–≤"}

# –ú–∞–ø–ø–∏–Ω–≥ –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
SENTIMENT_MAP = {
    "–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ": "—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ",
    "–Ω–µ–≥–∞—Ç–∏–≤": "—Ä–∞–∑–¥—Ä–∞–∂–µ–Ω–∏–µ", 
    "–ø–æ–∑–∏—Ç–∏–≤–Ω–æ–µ": "–ø–æ–∑–∏—Ç–∏–≤",
    "–ø–æ–∑–∏—Ç–∏–≤": "–ø–æ–∑–∏—Ç–∏–≤",
    "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ": "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ",
    "—Å–æ–º–Ω–µ–Ω–∏—è": "—Å–æ–º–Ω–µ–Ω–∏–µ"
}

def normalize_sentiment(sentiment: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∫ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω—ã–º –∑–Ω–∞—á–µ–Ω–∏—è–º"""
    if not sentiment:
        return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ"
    
    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É –∏ —É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã
    sentiment = sentiment.strip().lower()
    
    # –ü—Ä–∏–º–µ–Ω—è–µ–º –º–∞–ø–ø–∏–Ω–≥
    normalized = SENTIMENT_MAP.get(sentiment, sentiment)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ —Ä–∞–∑—Ä–µ—à–µ–Ω–Ω–æ–º —Å–ø–∏—Å–∫–µ
    if normalized not in ALLOWED_SENTIMENTS:
        return "–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ"
    
    return normalized


# –°–ª–æ–≤–∞—Ä—å –∫–∞—Ç–µ–≥–æ—Ä–∏–π —Ç–æ–≤–∞—Ä–æ–≤
PRODUCT_CATS = [
    ("—à–∏–Ω—ã|—Ä–µ–∑–∏–Ω[–∞—ã]|–∫–æ–ª–µ—Å", "—à–∏–Ω—ã"),
    ("—Å—Ç—Ä–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª|—Ü–µ–º–µ–Ω—Ç|–≥–∏–ø—Å–æ–∫–∞—Ä—Ç–æ–Ω|–∫–∏—Ä–ø–∏—á|—Å–∞–º–æ—Ä–µ–∑", "—Å—Ç—Ä–æ–π–º–∞—Ç–µ—Ä–∏–∞–ª—ã"),
    ("–¥–∏–≤–∞–Ω|–∫—Ä–æ–≤–∞—Ç—å|—à–∫–∞—Ñ|—Å—Ç–æ–ª|—Å—Ç—É–ª", "–º–µ–±–µ–ª—å"),
    ("—Å–º–∞—Ä—Ç—Ñ–æ–Ω|—Ç–µ–ª–µ—Ñ–æ–Ω|–Ω–æ—É—Ç–±—É–∫|–ø–ª–∞–Ω—à–µ—Ç|—Ç–µ–ª–µ–≤–∏–∑–æ—Ä|–Ω–∞—É—à–Ω–∏–∫", "—ç–ª–µ–∫—Ç—Ä–æ–Ω–∏–∫–∞"),
    ("–∑–∞–ø—á–∞—Å—Ç", "–∞–≤—Ç–æ–∑–∞–ø—á–∞—Å—Ç–∏"),
]

# –£–ª—É—á—à–µ–Ω–Ω—ã–µ —Ä–µ–≥–µ–∫—Å—ã –¥–ª—è —Ü–∏—Ç–∞—Ç
ROLE_RX = re.compile(r"^(–∫–ª–∏–µ–Ω—Ç|–æ–ø–µ—Ä–∞—Ç–æ—Ä)\s*:\s*(.*)$", re.IGNORECASE)

KW_FOR_QUOTES = re.compile(
    r"(–ø–≤–∑|–ø—É–Ω–∫—Ç –≤—ã–¥–∞—á–∏|–ø–æ—Å—Ç–∞–º–∞—Ç\w*|–∫—É—Ä—å–µ—Ä\w*|—Å–∞–º–æ–≤—ã–≤–æ–∑\w*|—Å–¥—ç–∫|boxberry|–±–æ–∫—Å–±–µ—Ä—Ä–∏|"
    r"–¥–æ—Å—Ç–∞–≤–∫\w+|–æ–Ω–ª–∞–π–Ω –æ–ø–ª–∞—Ç\w+|–≤–æ–∑–≤—Ä–∞—Ç\s*24|—Å—É–±—Å–∏–¥–∏—Ä–æ–≤–∞–Ω\w+|—Ç—Ä–µ–∫(?:-?–Ω–æ–º–µ—Ä)?)",
    re.IGNORECASE
)

# –†–µ–≥–µ–∫—Å—ã –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –±–∞—Ä—å–µ—Ä–æ–≤
QUESTION_RE = re.compile(r"(–∫–∞–∫|—á—Ç–æ|–∫—É–¥–∞|–∫–æ–≥–¥–∞)\b.*\?$", re.IGNORECASE)


def split_turns(raw: str):
    """–£—Å—Ç–æ–π—á–∏–≤—ã–π —Ä–∞–∑–±–æ—Ä –¥–∏–∞–ª–æ–≥–∞ –Ω–∞ —Ä–µ–ø–ª–∏–∫–∏"""
    turns = []
    for line in raw.splitlines():
        line = line.strip()
        if not line: 
            continue
        m = ROLE_RX.match(line)
        if m:
            turns.append({"speaker": m.group(1).lower(), "text": m.group(2).strip()})
        elif turns:
            turns[-1]["text"] += " " + line
    return turns


def sanitize_quote(s: str) -> str:
    """–ú–∞—Å–∫–∏—Ä–æ–≤–∞–Ω–∏–µ PII –≤ —Ü–∏—Ç–∞—Ç–∞—Ö"""
    s = re.sub(r'\b\d{10,11}\b','[masked-phone]', s)
    s = re.sub(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}','[masked-email]', s)
    return s.strip()


# –†–µ–≥–µ–∫—Å—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ —Ä–æ–ª–µ–π –∏ —á–∏—Å—Ç–∫–∏ —Ü–∏—Ç–∞—Ç
ROLE_RX = re.compile(r"^(–∫–ª–∏–µ–Ω—Ç|–æ–ø–µ—Ä–∞—Ç–æ—Ä)\s*:\s*(.*)$", re.IGNORECASE)
FILLERS = r"^(—É–≥—É|–∞–≥–∞|–Ω—É|–≤–æ—Ç|–∞|–º–º|—ç|—ç—ç|—Ç–∞–∫|–∫–æ—Ä–æ—á–µ|—Ç–∏–ø–∞|–∫–∞–∫ –±—ã|–ø—Ä–æ—Å—Ç–æ)\b"
MULTISPACE_RX = re.compile(r"\s+")
REPEAT_RX = re.compile(r"\b(\w+)(\s+\1\b)+", re.IGNORECASE)  # ¬´–¥–æ—Å—Ç–∞–≤–∫–∞ –¥–æ—Å—Ç–∞–≤–∫–∞¬ª ‚Üí ¬´–¥–æ—Å—Ç–∞–≤–∫–∞¬ª


def split_turns(raw: str):
    """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∏–∞–ª–æ–≥–∞ –Ω–∞ —Ä–µ–ø–ª–∏–∫–∏ —Å —Ä–æ–ª—è–º–∏"""
    turns = []
    for line in raw.splitlines():
        s = line.strip()
        if not s: 
            continue
        m = ROLE_RX.match(s)
        if m:
            turns.append({"speaker": m.group(1).lower(), "text": m.group(2).strip()})
        elif turns:
            turns[-1]["text"] += " " + s
    return turns


def clean_sentence(s: str, max_len=180) -> str:
    """–û—á–∏—Å—Ç–∫–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –æ—Ç –º—É—Å–æ—Ä–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª–∏–Ω—ã"""
    s = s.strip()
    # —É–±—Ä–∞—Ç—å —Å—Ç–∞—Ä—Ç–æ–≤—ã–µ –º–µ–∂–¥–æ–º–µ—Ç–∏—è –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫—É —Ä–∞–∑
    s = re.sub(rf"(?:{FILLERS}[\s,‚Äì‚Äî-]*)+", "", s, flags=re.IGNORECASE)
    # —Å—Ö–ª–æ–ø–Ω—É—Ç—å –ø–æ–≤—Ç–æ—Ä—ã —Å–ª–æ–≤
    s = REPEAT_RX.sub(r"\1", s)
    # –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é
    s = MULTISPACE_RX.sub(" ", s)
    # —É–±—Ä–∞—Ç—å —Ç–æ—á–∫–∏ –∏ –∑–∞–ø—è—Ç—ã–µ –≤ –Ω–∞—á–∞–ª–µ
    s = re.sub(r"^[.,\s]+", "", s)
    # —É–∫–æ—Ä–æ—Ç–∏—Ç—å –æ—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ
    if len(s) > max_len:
        s = s[:max_len-1].rstrip() + "‚Ä¶"
    return s


def split_to_sentences(text: str) -> list[str]:
    """–ì—Ä—É–±–∞—è —Ä—É—Å—Å–∫–∞—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –ø–æ .!? + –ø–µ—Ä–µ–Ω–æ—Å–∞–º"""
    parts = re.split(r"(?<=[\.\?\!])\s+|\n+", text)
    return [p.strip() for p in parts if p.strip()]


def postprocess_quotes(quotes: list[dict], limit=3) -> list[dict]:
    """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ü–∏—Ç–∞—Ç: —á–∏—Å—Ç–∫–∞, –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è, —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è"""
    out = []
    seen = set()
    for q in quotes:
        for sent in split_to_sentences(q["quote"]):
            sent = clean_sentence(sent)
            if len(sent) < 8:
                continue
            key = sent.lower()
            if key in seen:
                continue  # –¥–µ–¥—É–ø
            seen.add(key)
            out.append({"quote": sent, "speaker": "–ö–ª–∏–µ–Ω—Ç"})
            if len(out) >= limit:
                return out
    return out


def mark_source_role(items, speaker):
    """–†–∞–∑–º–µ—Ç–∫–∞ —Ä–æ–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –¥–ª—è —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
    for it in items or []:
        it["source_role"] = "client" if speaker == "–∫–ª–∏–µ–Ω—Ç" else "operator"


def pick_client_quotes(turns, limit=3):
    """–ì–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–±–æ—Ä —Ü–∏—Ç–∞—Ç –∫–ª–∏–µ–Ω—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π —á–∏—Å—Ç–∫–æ–π"""
    raw = []
    # 1) –∫–ª–∏–µ–Ω—Ç—Å–∫–∏–µ —Å –¥–æ—Å—Ç–∞–≤–æ—á–Ω—ã–º–∏ —Ç–µ—Ä–º–∏–Ω–∞–º–∏
    for t in turns:
        if t["speaker"] != "–∫–ª–∏–µ–Ω—Ç": 
            continue
        tx = t["text"].strip()
        if KW_FOR_QUOTES.search(tx) and len(tx) >= 8:
            raw.append(mask_pii(tx))
    
    # 2) —Å–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è + –æ—á–∏—Å—Ç–∫–∞ + –¥–µ–¥—É–ø
    out, seen = set(), []
    for q in raw:
        for sent in split_to_sentences(q):
            sent = clean_sentence(sent)
            if len(sent) < 10: 
                continue
            key = sent.lower()
            if key in out: 
                continue
            out.add(key)
            seen.append({"quote": sent, "speaker": "–ö–ª–∏–µ–Ω—Ç"})
            if len(seen) >= limit: 
                return seen
    return seen


def guess_product_category(text: str) -> str | None:
    """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ —Ç–æ–≤–∞—Ä–∞ –ø–æ —Ç–µ–∫—Å—Ç—É –¥–∏–∞–ª–æ–≥–∞"""
    import re
    t = text.lower()
    for rx, cat in PRODUCT_CATS:
        if re.search(rx, t):
            return cat
    return None

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI
if settings.openai_api_key:
    openai.api_key = settings.openai_api_key


def clean_quote(q: str) -> str:
    """–û—á–∏—Å—Ç–∫–∞ —Ü–∏—Ç–∞—Ç—ã –æ—Ç PII"""
    import re
    q = q.strip()
    # –ü—Ä–æ—Å—Ç–∞—è –∑–∞—â–∏—Ç–∞ PII
    q = re.sub(r'\b\d{11}\b', '[masked-phone]', q)
    q = re.sub(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', '[masked-email]', q)
    return q


def pick_client_quotes(turns, limit=3):
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç –∫–ª–∏–µ–Ω—Ç–∞ —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –ª–æ–≥–∏–∫–æ–π"""
    import re
    
    # –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ü–∏—Ç–∞—Ç
    KW_FOR_QUOTES = re.compile(
        r"(–ø–≤–∑|–ø—É–Ω–∫—Ç –≤—ã–¥–∞—á–∏|–ø–æ—Å—Ç–∞–º–∞—Ç\w*|–∫—É—Ä—å–µ—Ä\w*|—Å–∞–º–æ–≤—ã–≤–æ–∑\w*|–¥–æ—Å—Ç–∞–≤–∫\w+|–≤–æ–∑–≤—Ä–∞—Ç 24|—Å—É–±—Å–∏–¥–∏—Ä–æ–≤–∞–Ω\w+|–æ–Ω–ª–∞–π–Ω –æ–ø–ª–∞—Ç\w+)", 
        re.IGNORECASE
    )
    
    def sanitize_quote(s: str) -> str:
        s = re.sub(r'\b\d{10,11}\b', '[masked-phone]', s)
        s = re.sub(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', '[masked-email]', s)
        return s.strip()
    
    out = []
    
    # 1) –ö–ª–∏–µ–Ω—Ç—Å–∫–∏–µ —Ä–µ–ø–ª–∏–∫–∏ —Å –¥–æ—Å—Ç–∞–≤–æ—á–Ω—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
    for t in turns:
        if t.get("speaker", "").lower() != "–∫–ª–∏–µ–Ω—Ç":
            continue
        tx = t.get("text", "").strip()
        if KW_FOR_QUOTES.search(tx) and 8 <= len(tx) <= 280:
            out.append({"quote": sanitize_quote(tx), "speaker": "–ö–ª–∏–µ–Ω—Ç"})
            if len(out) >= limit:
                return out
    
    # 2) –°–æ—Å–µ–¥–Ω–∏–µ –∫ –æ–ø–µ—Ä–∞—Ç–æ—Ä—Å–∫–∏–º –¥–æ—Å—Ç–∞–≤–æ—á–Ω—ã–º —Ä–µ–ø–ª–∏–∫–∞–º
    for i, t in enumerate(turns):
        if t.get("speaker", "").lower() == "–æ–ø–µ—Ä–∞—Ç–æ—Ä" and KW_FOR_QUOTES.search(t.get("text", "")):
            for j in (i-1, i+1):
                if 0 <= j < len(turns) and turns[j].get("speaker", "").lower() == "–∫–ª–∏–µ–Ω—Ç":
                    tx = turns[j].get("text", "").strip()
                    if 8 <= len(tx) <= 280:
                        out.append({"quote": sanitize_quote(tx), "speaker": "–ö–ª–∏–µ–Ω—Ç"})
                        if len(out) >= limit:
                            return out
    
    return out


def has_delivery_cue(citations):
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–æ—Å—Ç–∞–≤–∫–∏ –≤ —Ü–∏—Ç–∞—Ç–∞—Ö"""
    for citation in citations:
        quote_lower = citation.get("quote", "").lower()
        if any(keyword in quote_lower for keyword in DELIVERY_KEYWORDS):
            return True
    return False


@retry(
    stop=stop_after_attempt(settings.max_retries),
    wait=wait_exponential(multiplier=settings.retry_backoff_sec)
)
def extract_entities_openai(text: str, dialog_id: str) -> Dict[str, Any]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ OpenAI API —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ —Ü–∏—Ç–∞—Ç–∞–º–∏ –∫–ª–∏–µ–Ω—Ç–∞"""
    try:
        from openai import OpenAI
        client = OpenAI(api_key=settings.openai_api_key)
        
        # –ü–∞—Ä—Å–∏–º –¥–∏–∞–ª–æ–≥ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–∏—Ç–∞—Ç –∫–ª–∏–µ–Ω—Ç–∞
        try:
            turns = split_turns(text)
            client_quotes = pick_client_quotes(turns, limit=3)
            low_evidence = (len(client_quotes) == 0)
        except:
            client_quotes = []
            low_evidence = True
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        with open("prompts/extract_entities.txt", "r", encoding="utf-8") as f:
            updated_prompt = f.read()
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
        params = {
            "model": settings.model_extract,
            "messages": [
                {"role": "system", "content": updated_prompt},
                {"role": "user", "content": f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —ç—Ç–æ—Ç –¥–∏–∞–ª–æ–≥:\n\n{text}"}
            ],
            "response_format": {"type": "json_object"},
            "max_completion_tokens": 1000
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ç–æ–ª—å–∫–æ –¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏—Ö –º–æ–¥–µ–ª–µ–π
        if "o3" not in settings.model_extract:
            params.update({
                "temperature": 0.2,
                "top_p": 0.9
            })
        
        try:
            response = client.chat.completions.create(**params)
        except Exception as e:
            # Fallback –Ω–∞ gpt-4o-mini –µ—Å–ª–∏ o3-mini –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç
            if "o3" in settings.model_extract:
                params["model"] = "gpt-4o-mini"
                params.update({
                    "temperature": 0.2,
                    "top_p": 0.9
                })
                response = client.chat.completions.create(**params)
            else:
                raise e
        
        content = response.choices[0].message.content
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ—Ç–≤–µ—Ç –Ω–µ –ø—É—Å—Ç–æ–π
        if not content or not content.strip():
            logger.warning(f"–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç OpenAI –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ {dialog_id}")
            return {
                "delivery_types": [],
                "barriers": [],
                "ideas": [],
                "signals": [],
                "citations": client_quotes
            }
        
        # –û—á–∏—â–∞–µ–º JSON –æ—Ç markdown –±–ª–æ–∫–æ–≤
        cleaned_content = content.strip()
        if cleaned_content.startswith("```json"):
            cleaned_content = cleaned_content[7:]  # –£–±–∏—Ä–∞–µ–º ```json
        if cleaned_content.endswith("```"):
            cleaned_content = cleaned_content[:-3]  # –£–±–∏—Ä–∞–µ–º ```
        cleaned_content = cleaned_content.strip()
        
        # –ü—ã—Ç–∞–µ–º—Å—è –ø–∞—Ä—Å–∏—Ç—å JSON
        try:
            result = json.loads(cleaned_content)
            
            # –ó–∞–º–µ–Ω—è–µ–º —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ü–∏—Ç–∞—Ç—ã –Ω–∞ —Ä–µ–∞–ª—å–Ω—ã–µ
            result["citations"] = client_quotes
            result["low_evidence"] = low_evidence
            
            # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å
            if "sentiment" in result:
                result["sentiment"] = normalize_sentiment(result["sentiment"])
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –±–∞—Ä—å–µ—Ä—ã –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –¥–æ—Å—Ç–∞–≤–∫–∏
            if result.get("delivery_discussed") is True:
                has_delivery_cue_in_citations = has_delivery_cue(client_quotes)
                if not has_delivery_cue_in_citations and result.get("barriers"):
                    # –ï—Å–ª–∏ –Ω–µ—Ç —è–≤–Ω–æ–π –∫–ª–∏–µ–Ω—Ç—Å–∫–æ–π —Ä–µ–ø–ª–∏–∫–∏ –ø—Ä–æ –¥–æ—Å—Ç–∞–≤–∫—É - —É–±–∏—Ä–∞–µ–º –±–∞—Ä—å–µ—Ä—ã
                    result["barriers"] = []
            
            return result
        except json.JSONDecodeError as json_err:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ JSON –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ {dialog_id}: {json_err}. –û—Ç–≤–µ—Ç: {cleaned_content[:200]}...")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–º–µ—Å—Ç–æ –ø–∞–¥–µ–Ω–∏—è
            return {
                "delivery_types": [],
                "barriers": [],
                "ideas": [],
                "signals": [],
                "citations": client_quotes
            }
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ OpenAI API –¥–ª—è –¥–∏–∞–ª–æ–≥–∞ {dialog_id}: {e}")
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤–º–µ—Å—Ç–æ –ø–∞–¥–µ–Ω–∏—è
        return {
            "delivery_types": [],
            "barriers": [],
            "ideas": [],
            "signals": [],
            "citations": []
        }


def extract_entities_simple(text: str, dialog_id: str) -> Dict[str, Any]:
    """–ü—Ä–æ—Å—Ç–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    text_lower = text.lower()
    
    # –ü—Ä–æ—Å—Ç—ã–µ —ç–≤—Ä–∏—Å—Ç–∏–∫–∏ –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è
    barriers = []
    ideas = []
    signals = []
    
    # –ë–∞—Ä—å–µ—Ä—ã
    if "–º–∞–ª–æ –ø–≤–∑" in text_lower or "–Ω–µ—Ç –ø—É–Ω–∫—Ç–∞" in text_lower:
        barriers.append("–º–∞–ª–æ –ø–≤–∑")
    if "–¥–æ—Ä–æ–≥–æ" in text_lower or "–¥–æ—Ä–æ–≥–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞" in text_lower:
        barriers.append("–¥–æ—Ä–æ–≥–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞")
    if "–Ω–µ –ø–æ–¥–Ω–∏–º–∞–µ—Ç—Å—è" in text_lower or "–Ω–µ –¥–æ—Ö–æ–¥–∏—Ç" in text_lower:
        barriers.append("–∫—É—Ä—å–µ—Ä –Ω–µ –ø–æ–¥–Ω–∏–º–∞–µ—Ç—Å—è")
    
    # –ò–¥–µ–∏
    if "—Å–∫–∏–¥–∫–∞" in text_lower or "–¥–µ—à–µ–≤–ª–µ" in text_lower:
        ideas.append("—Å–∫–∏–¥–∫–∞ –Ω–∞ –¥–æ—Å—Ç–∞–≤–∫—É")
    if "–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞" in text_lower:
        ideas.append("–±–µ—Å–ø–ª–∞—Ç–Ω–∞—è –¥–æ—Å—Ç–∞–≤–∫–∞")
    
    # –°–∏–≥–Ω–∞–ª—ã
    if "–Ω–µ –ø–æ–Ω–∏–º–∞—é" in text_lower or "–Ω–µ —Ä–∞–∑–æ–±—Ä–∞–ª—Å—è" in text_lower:
        signals.append("–Ω–µ–∑–Ω–∞–Ω–∏–µ")
    if "—Å–ª–æ–∂–Ω–æ" in text_lower or "–∑–∞–ø—É—Ç–∞–Ω–Ω–æ" in text_lower:
        signals.append("—Å–ª–æ–∂–Ω–æ—Å—Ç—å")
    
    # –¢–∏–ø—ã –¥–æ—Å—Ç–∞–≤–∫–∏
    delivery_types = []
    if "–ø–≤–∑" in text_lower or "–ø—É–Ω–∫—Ç –≤—ã–¥–∞—á–∏" in text_lower:
        delivery_types.append("–ü–í–ó")
    if "–∫—É—Ä—å–µ—Ä" in text_lower:
        delivery_types.append("–∫—É—Ä—å–µ—Ä—Å–∫–∞—è")
    if "—Å–∞–º–æ–≤—ã–≤–æ–∑" in text_lower or "–∑–∞–±–µ—Ä—É —Å–∞–º" in text_lower:
        delivery_types.append("—Å–∞–º–æ–≤—ã–≤–æ–∑")
    
    return {
        "dialog_id": dialog_id,
        "delivery_discussed": True,
        "delivery_types": delivery_types,
        "barriers": barriers,
        "ideas": ideas,
        "signals": signals,
        "citations": [],
        "region": "",
        "segment": "",
        "product_category": "",
        "sentiment": "",
        "extras": {}
    }


def process_dialog_batch(dialogs: List[Dict[str, Any]], delivery_dialog_ids: Set[str]) -> List[DialogExtraction]:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ –¥–∏–∞–ª–æ–≥–æ–≤"""
    results = []
    
    for dialog in dialogs:
        try:
            dialog_id = str(dialog[settings.col_id])
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –¥–∏–∞–ª–æ–≥–∏ –±–µ–∑ –¥–æ—Å—Ç–∞–≤–∫–∏
            if dialog_id not in delivery_dialog_ids:
                continue
            
            text = str(dialog[settings.col_text])
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
            if settings.use_openai and settings.openai_api_key:
                extraction = extract_entities_openai(text, dialog_id)
            else:
                extraction = extract_entities_simple(text, dialog_id)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            try:
                # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–∏—Ç–∞—Ç –∫–ª–∏–µ–Ω—Ç–∞
                turns = split_turns(text)
                citations = pick_client_quotes(turns, limit=3)
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º low_evidence
                low_evidence = len(citations) == 0
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–∏–ø–æ–≤ –¥–ª—è –ø–æ–ª–µ–π List[str]
                def ensure_string_list(value, field_name):
                    """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ —Å–ø–∏—Å–æ–∫ —Å—Ç—Ä–æ–∫"""
                    if not isinstance(value, list):
                        logger.warning(f"–ü–æ–ª–µ {field_name} –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —Å–ø–∏—Å–∫–æ–º: {type(value)}")
                        return []
                    
                    result = []
                    for item in value:
                        if isinstance(item, str):
                            result.append(item)
                        elif isinstance(item, (int, float)):
                            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —á–∏—Å–ª–∞ –≤ —Å—Ç—Ä–æ–∫–∏
                            result.append(str(item))
                        elif isinstance(item, dict):
                            # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –∏–∑ —Å–ª–æ–≤–∞—Ä—è (–µ—Å–ª–∏ –µ—Å—Ç—å –ø–æ–ª–µ 'text' –∏–ª–∏ 'name')
                            text = item.get('text') or item.get('name') or str(item)
                            result.append(str(text))
                        else:
                            logger.warning(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π —Ç–∏–ø –≤ {field_name}: {type(item)}")
                            result.append(str(item))
                    
                    return result
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ç–æ–≤–∞—Ä–∞, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–∞
                product_category = extraction.get("product_category", "")
                if not product_category:
                    product_category = guess_product_category(text) or ""
                
                result = DialogExtraction(
                    dialog_id=dialog_id,
                    delivery_discussed=extraction.get("delivery_discussed", True),
                    delivery_types=ensure_string_list(extraction.get("delivery_types", []), "delivery_types"),
                    barriers=ensure_string_list(extraction.get("barriers", []), "barriers"),
                    ideas=ensure_string_list(extraction.get("ideas", []), "ideas"),
                    signals=ensure_string_list(extraction.get("signals", []), "signals"),
                    citations=citations,
                    region=extraction.get("region", ""),
                    segment=extraction.get("segment", ""),
                    product_category=product_category,
                    sentiment=extraction.get("sentiment", ""),
                    extras=extraction.get("extras", {})
                )
                
                # –î–æ–±–∞–≤–ª—è–µ–º low_evidence –≤ extras
                result.extras["low_evidence"] = low_evidence
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –¥–∏–∞–ª–æ–≥–∞ {dialog_id}: {e}")
                continue
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∏–∞–ª–æ–≥–∞ {dialog.get(settings.col_id, 'unknown')}: {e}")
            continue
    
    return results


def load_delivery_detections() -> Set[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ ID –¥–∏–∞–ª–æ–≥–æ–≤ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π –∏–∑ Stage 1"""
    delivery_file = Path("artifacts/stage1_delivery.jsonl")
    
    if not delivery_file.exists():
        logger.error(f"‚ùå –§–∞–π–ª {delivery_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 1 —Å–Ω–∞—á–∞–ª–∞.")
        return set()
    
    delivery_ids = set()
    
    with open(delivery_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                if data.get("delivery_discussed", False):
                    delivery_ids.add(data["dialog_id"])
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å—Ç—Ä–æ–∫–∏: {e}")
                continue
    
    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(delivery_ids)} –¥–∏–∞–ª–æ–≥–æ–≤ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π")
    return delivery_ids


def load_valid_samples() -> Set[str]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ Stage 1.5"""
    # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ Stage 1.5
    sampling_file = Path("artifacts/stage1_5_sampling.jsonl")
    
    if sampling_file.exists():
        logger.info("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤ –∏–∑ Stage 1.5...")
        valid_ids = set()
        
        with open(sampling_file, 'r', encoding='utf-8') as f:
            for line in f:
                try:
                    data = json.loads(line.strip())
                    if data.get("valid_sample", False):
                        valid_ids.add(data["dialog_id"])
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –æ–±—Ä–∞–∑—Ü–∞: {e}")
                    continue
        
        logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(valid_ids)} –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤")
        
        # –ï—Å–ª–∏ Stage 1.5 –Ω–µ –¥–∞–ª –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º Stage 1
        if len(valid_ids) == 0:
            logger.warning("‚ö†Ô∏è Stage 1.5 –Ω–µ –¥–∞–ª –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤, –∏—Å–ø–æ–ª—å–∑—É–µ–º Stage 1...")
            return load_delivery_detections()
        
        return valid_ids
    
    # Fallback –Ω–∞ Stage 1
    logger.warning("‚ö†Ô∏è Stage 1.5 –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º Stage 1...")
    delivery_file = Path("artifacts/stage1_delivery.jsonl")
    
    if not delivery_file.exists():
        logger.error(f"‚ùå –§–∞–π–ª {delivery_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 1 —Å–Ω–∞—á–∞–ª–∞.")
        return set()
    
    delivery_ids = set()
    
    with open(delivery_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                if data.get("delivery_discussed", False):
                    delivery_ids.add(data["dialog_id"])
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Å—Ç—Ä–æ–∫–∏: {e}")
                continue
    
    return delivery_ids


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Stage 2"""
    logger.info("üöÄ Stage 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ artifacts
    artifacts_dir = Path("artifacts")
    artifacts_dir.mkdir(exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞–ª–∏–¥–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤
    valid_dialog_ids = load_valid_samples()
    logger.info(f"üìä –ù–∞–π–¥–µ–Ω–æ {len(valid_dialog_ids)} –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤")
    
    if not valid_dialog_ids:
        logger.error("‚ùå –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    logger.info(f"üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {settings.xlsx_path}")
    try:
        df = pd.read_excel(settings.xlsx_path)
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} –¥–∏–∞–ª–æ–≥–æ–≤")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return
    
    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞–º–∏
    results = []
    total_dialogs = len(df)
    
    logger.info(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∏–∞–ª–æ–≥–æ–≤ —Å –¥–æ—Å—Ç–∞–≤–∫–æ–π –±–∞—Ç—á–∞–º–∏ –ø–æ {settings.batch_size}")
    
    for i in tqdm(range(0, total_dialogs, settings.batch_size), desc="–û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–µ–π"):
        batch_df = df.iloc[i:i + settings.batch_size]
        batch_dialogs = batch_df.to_dict('records')
        
        batch_results = process_dialog_batch(batch_dialogs, valid_dialog_ids)
        results.extend(batch_results)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_file = artifacts_dir / "stage2_extracted.jsonl"
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        for result in results:
            f.write(json.dumps(result.dict()) + '\n')
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_barriers = sum(len(r.barriers) for r in results)
    total_ideas = sum(len(r.ideas) for r in results)
    total_signals = sum(len(r.signals) for r in results)
    total_citations = sum(len(r.citations) for r in results)
    
    logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ Stage 2:")
    logger.info(f"  –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–∏–∞–ª–æ–≥–æ–≤: {len(results)}")
    logger.info(f"  –í—Å–µ–≥–æ –±–∞—Ä—å–µ—Ä–æ–≤: {total_barriers}")
    logger.info(f"  –í—Å–µ–≥–æ –∏–¥–µ–π: {total_ideas}")
    logger.info(f"  –í—Å–µ–≥–æ —Å–∏–≥–Ω–∞–ª–æ–≤: {total_signals}")
    logger.info(f"  –í—Å–µ–≥–æ —Ü–∏—Ç–∞—Ç: {total_citations}")
    
    logger.info("‚úÖ Stage 2 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    main()
