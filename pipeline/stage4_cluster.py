#!/usr/bin/env python3
"""
Stage 4: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫
–ì—Ä—É–ø–ø–∏—Ä—É–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –±–∞—Ä—å–µ—Ä—ã –∏ –∏–¥–µ–∏ –≤ –∫–ª–∞—Å—Ç–µ—Ä—ã
"""

import json
import logging
import re
from pathlib import Path
from collections import Counter, defaultdict
from typing import List, Dict, Any, Set, Tuple
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.metrics.pairwise import cosine_similarity

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from config import settings
from models.validation import DialogExtraction, Cluster, ClusterVariant, ClusterSlices, ClustersData
from prompts import STAGE_CONFIG, CLUSTERING_STOPWORDS

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –∞–≤—Ç–æ-–ª–µ–π–±–ª–æ–≤
TRASH = {"–≤–æ–ø—Ä–æ—Å", "–ø—Ä–æ–±–ª–µ–º–∞", "–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ", "–∫–ª–∏–µ–Ω—Ç–æ–≤", "–∫–ª–∏–µ–Ω—Ç–∞", "–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã"}
BAD = re.compile(r"(–ø–∏–∑–¥|—Ö–µ—Ä|–±–ª–∏–Ω|—á–µ—Ä—Ç|—Ö—Ä–µ–Ω—å)", re.IGNORECASE)
WORD_RX = re.compile(r"[A-Za-z–ê-–Ø–∞-—è0-9\-]+")
QUESTION_RX = re.compile(r"\b(–∫–∞–∫|–ø–æ—á–µ–º—É|–≥–¥–µ|—á—Ç–æ –¥–µ–ª–∞—Ç—å|—á—Ç–æ –¥–∞–ª—å—à–µ)\b", re.IGNORECASE)

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def top_terms(texts: List[str], topn: int = 5) -> List[str]:
    """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–æ–ø-N —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ —Ç–µ–∫—Å—Ç–æ–≤"""
    cnt = Counter()
    for t in texts:
        for w in WORD_RX.findall(t.lower()):
            if len(w) < 3: 
                continue
            if w in TRASH:
                continue
            cnt[w] += 1
    return [w for w, _ in cnt.most_common(topn)]


def norm(s: str) -> str:
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞"""
    s = s.replace("—ë", "–µ").lower().strip()
    s = re.sub(r"\s+", " ", s)
    return s


def short_label(variants: List[str]) -> str:
    """–ö–æ—Ä–æ—Ç–∫–∏–π –∏ –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–π –ª–µ–π–±–ª –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    cand = [re.sub(r"\s+", " ", v.strip().lower()) for v in variants if v.strip()]
    cnt = Counter(cand)
    
    # 1) –≤–∑—è—Ç—å —á–∞—Å—Ç—É—é —Ñ—Ä–∞–∑—É –¥–æ 5 —Å–ª–æ–≤, –±–µ–∑ –º—É—Å–æ—Ä–∞ –∏ –º–∞—Ç–∞
    for v, _ in cnt.most_common():
        if BAD.search(v): 
            continue
        if any(t in v for t in TRASH): 
            continue
        if len(v.split()) <= 5:
            return v.capitalize()
    
    # 2) fallback –ø–æ —Ç–æ–ø-—Ç–µ—Ä–º–∞–º
    terms = top_terms(cand, topn=3)
    return (" ".join(terms)).capitalize() if terms else "–ö–ª–∞—Å—Ç–µ—Ä"


def promote_questions_to_barriers(item):
    """–ü—Ä–æ–º–æ –≤–æ–ø—Ä–æ—Å–æ–≤ –≤ –±–∞—Ä—å–µ—Ä—ã"""
    txt = (item.get("evidence_span") or "").lower()
    if QUESTION_RX.search(txt):
        labs = set(item.get("labels", {}).get("barrier", []))
        labs.update({"–Ω–µ–ø–æ–Ω–∏–º–∞–Ω–∏–µ –ø—Ä–æ—Ü–µ—Å—Å–∞ –¥–æ—Å—Ç–∞–≤–∫–∏"})
        item.setdefault("labels", {}).setdefault("barrier", [])
        item["labels"]["barrier"] = list(labs)
    return item


def auto_label_cluster(variants: List[str]) -> str:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é"""
    return short_label(variants)


def auto_label(variants: List[str]) -> str:
    """–û–±—Ä–∞—Ç–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –Ω–æ–≤—É—é —Ñ—É–Ω–∫—Ü–∏—é"""
    return auto_label_cluster(variants)


def cluster_texts(texts: List[str]) -> List[int]:
    """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤ —Å –¥—Ä–æ–±–ª–µ–Ω–∏–µ–º –±–æ–ª—å—à–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä–æ–≤"""
    if len(texts) < 2:
        return [0] * len(texts)
    
    # TF-IDF –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏—è
    vectorizer = TfidfVectorizer(
        ngram_range=(1, 2), 
        max_df=0.6, 
        min_df=2,
        stop_words=None
    )
    X = vectorizer.fit_transform(texts)
    
    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞ –¥–ª—è –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    k = min(max(2, len(texts) // 8), STAGE_CONFIG["max_clusters"])
    
    # KMeans –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    km = KMeans(n_clusters=k, n_init="auto", random_state=42)
    labels = km.fit_predict(X)
    
    # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–º—É —Ä–∞–∑–º–µ—Ä—É –∫–ª–∞—Å—Ç–µ—Ä–∞
    MIN_CLUSTER = STAGE_CONFIG["min_cluster_size"]
    clusters = {i: [] for i in range(k)}
    for text, label in zip(texts, labels):
        clusters[label].append(text)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø–ª–æ—Ç–Ω—ã–µ –∏ —Ä–µ–¥–∫–∏–µ –∫–ª–∞—Å—Ç–µ—Ä—ã
    dense_clusters = [c for c in clusters.values() if len(c) >= MIN_CLUSTER]
    rare_cases = [c for c in clusters.values() if len(c) < MIN_CLUSTER]
    
    # –ü–µ—Ä–µ—Å—á–µ—Ç –ª–µ–π–±–ª–æ–≤
    new_labels = []
    label_map = {}
    new_label = 0
    
    for i, text in enumerate(texts):
        old_label = labels[i]
        if old_label not in label_map:
            if len(clusters[old_label]) >= MIN_CLUSTER:
                label_map[old_label] = new_label
                new_label += 1
            else:
                # –†–µ–¥–∫–∏–µ —Å–ª—É—á–∞–∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –æ–¥–∏–Ω –∫–ª–∞—Å—Ç–µ—Ä
                label_map[old_label] = -1
        new_labels.append(label_map[old_label])
    
    return new_labels


def create_embeddings(texts: List[str]) -> np.ndarray:
    """–°–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤"""
    if not texts:
        return np.array([])
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º TF-IDF –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã
    vectorizer = TfidfVectorizer(
        max_features=1000,
        stop_words=None,  # –ù–µ —É–±–∏—Ä–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
        ngram_range=(1, 2)
    )
    
    try:
        tfidf_matrix = vectorizer.fit_transform(texts)
        return tfidf_matrix.toarray()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {e}")
        return np.array([])


def cluster_texts(texts: List[str], min_clusters: int = 2, max_clusters: int = 10) -> List[int]:
    """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–æ–≤"""
    if len(texts) < 2:
        return [0] * len(texts)
    
    embeddings = create_embeddings(texts)
    if embeddings.size == 0:
        return [0] * len(texts)
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    n_clusters = min(max(min_clusters, len(texts) // 3), max_clusters, len(texts))
    
    try:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(embeddings)
        return cluster_labels.tolist()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
        return [0] * len(texts)


def create_cluster_variants(texts: List[str], counts: List[int]) -> List[ClusterVariant]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É –∏ —Å—É–º–º–∏—Ä—É–µ–º —Å—á–µ—Ç—á–∏–∫–∏
    text_counts = defaultdict(int)
    for text, count in zip(texts, counts):
        text_counts[text] += count
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
    sorted_variants = sorted(text_counts.items(), key=lambda x: x[1], reverse=True)
    
    return [ClusterVariant(text=text, count=count) for text, count in sorted_variants]


def create_cluster_slices(dialogs: List[DialogExtraction], cluster_texts: List[str]) -> ClusterSlices:
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å—Ä–µ–∑–æ–≤ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    regions = Counter()
    segments = Counter()
    product_categories = Counter()
    delivery_types = Counter()
    sentiment = Counter()
    
    for dialog in dialogs:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —Ç–µ–∫—Å—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∞ –≤ –¥–∏–∞–ª–æ–≥–µ
        dialog_texts = dialog.barriers + dialog.ideas + dialog.signals
        if any(text in dialog_texts for text in cluster_texts):
            if dialog.region:
                regions[dialog.region] += 1
            if dialog.segment:
                segments[dialog.segment] += 1
            if dialog.product_category:
                product_categories[dialog.product_category] += 1
            for delivery_type in dialog.delivery_types:
                delivery_types[delivery_type] += 1
            if dialog.sentiment:
                sentiment[dialog.sentiment] += 1
    
    return ClusterSlices(
        regions=dict(regions),
        segments=dict(segments),
        product_categories=dict(product_categories),
        delivery_types=dict(delivery_types),
        sentiment=dict(sentiment)
    )


def cluster_category(dialogs: List[DialogExtraction], category: str) -> List[Cluster]:
    """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ (–±–∞—Ä—å–µ—Ä—ã, –∏–¥–µ–∏, —Å–∏–≥–Ω–∞–ª—ã)"""
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã –∏ –∏—Ö —Å—á–µ—Ç—á–∏–∫–∏
    text_counts = Counter()
    text_to_dialogs = defaultdict(list)
    
    for dialog in dialogs:
        texts = getattr(dialog, category, [])
        for text in texts:
            text_counts[text] += 1
            text_to_dialogs[text].append(dialog)
    
    if not text_counts:
        return []
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
    texts = list(text_counts.keys())
    counts = [text_counts[text] for text in texts]
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    cluster_labels = cluster_texts(texts)
    
    # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º
    clusters = defaultdict(list)
    for text, label in zip(texts, cluster_labels):
        clusters[label].append(text)
    
    # –°–æ–∑–¥–∞–µ–º –æ–±—ä–µ–∫—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    result_clusters = []
    
    for cluster_id, cluster_texts_list in clusters.items():
        if not cluster_texts_list:
            continue
        
        # –°–æ–∑–¥–∞–µ–º –≤–∞—Ä–∏–∞–Ω—Ç—ã —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫
        cluster_counts = [text_counts[text] for text in cluster_texts_list]
        variants = create_cluster_variants(cluster_texts_list, cluster_counts)
        
        # –°–æ–±–∏—Ä–∞–µ–º ID –¥–∏–∞–ª–æ–≥–æ–≤
        dialog_ids = set()
        for text in cluster_texts_list:
            for dialog in text_to_dialogs[text]:
                dialog_ids.add(dialog.dialog_id)
        
        # –°–æ–∑–¥–∞–µ–º —Å—Ä–µ–∑—ã –¥–∞–Ω–Ω—ã—Ö
        cluster_dialogs = [dialog for dialog in dialogs if dialog.dialog_id in dialog_ids]
        slices = create_cluster_slices(cluster_dialogs, cluster_texts_list)
        
        # –ù–∞–∑–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–∞ - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        cluster_name = auto_label(cluster_texts_list)
        
        cluster = Cluster(
            name=cluster_name,
            variants=variants,
            dialog_ids=list(dialog_ids),
            slices=slices
        )
        
        result_clusters.append(cluster)
    
    # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –æ–±—â–µ–º—É –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —É–ø–æ–º–∏–Ω–∞–Ω–∏–π
    result_clusters.sort(key=lambda c: sum(v.count for v in c.variants), reverse=True)
    
    return result_clusters


def load_normalized_dialogs() -> List[DialogExtraction]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤"""
    normalized_file = Path("artifacts/stage3_normalized.jsonl")
    
    if not normalized_file.exists():
        logger.error(f"‚ùå –§–∞–π–ª {normalized_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 3 —Å–Ω–∞—á–∞–ª–∞.")
        return []
    
    dialogs = []
    
    with open(normalized_file, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                data = json.loads(line.strip())
                dialog = DialogExtraction(**data)
                dialogs.append(dialog)
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –¥–∏–∞–ª–æ–≥–∞: {e}")
                continue
    
    return dialogs


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Stage 4"""
    logger.info("üöÄ Stage 4: –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ artifacts
    artifacts_dir = Path("artifacts")
    artifacts_dir.mkdir(exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤
    dialogs = load_normalized_dialogs()
    logger.info(f"üìä –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dialogs)} –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
    
    if not dialogs:
        logger.error("‚ùå –ù–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
        return
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
    logger.info("üîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –±–∞—Ä—å–µ—Ä–æ–≤...")
    barrier_clusters = cluster_category(dialogs, "barriers")
    
    logger.info("üîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∏–¥–µ–π...")
    idea_clusters = cluster_category(dialogs, "ideas")
    
    logger.info("üîÑ –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å–∏–≥–Ω–∞–ª–æ–≤...")
    signal_clusters = cluster_category(dialogs, "signals")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    clusters_data = ClustersData(
        barriers=barrier_clusters,
        ideas=idea_clusters,
        signals=signal_clusters
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    output_file = artifacts_dir / "stage4_clusters.json"
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {output_file}")
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(clusters_data.dict(), f, ensure_ascii=False, indent=2)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_barrier_mentions = sum(sum(v.count for v in c.variants) for c in barrier_clusters)
    total_idea_mentions = sum(sum(v.count for v in c.variants) for c in idea_clusters)
    total_signal_mentions = sum(sum(v.count for v in c.variants) for c in signal_clusters)
    
    logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏:")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –±–∞—Ä—å–µ—Ä–æ–≤: {len(barrier_clusters)}")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–¥–µ–π: {len(idea_clusters)}")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤: {len(signal_clusters)}")
    logger.info(f"  –í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –±–∞—Ä—å–µ—Ä–æ–≤: {total_barrier_mentions}")
    logger.info(f"  –í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π –∏–¥–µ–π: {total_idea_mentions}")
    logger.info(f"  –í—Å–µ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å–∏–≥–Ω–∞–ª–æ–≤: {total_signal_mentions}")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ø-–∫–ª–∞—Å—Ç–µ—Ä—ã
    if barrier_clusters:
        logger.info("üîç –¢–æ–ø-3 –∫–ª–∞—Å—Ç–µ—Ä–∞ –±–∞—Ä—å–µ—Ä–æ–≤:")
        for i, cluster in enumerate(barrier_clusters[:3]):
            total_mentions = sum(v.count for v in cluster.variants)
            logger.info(f"  {i+1}. {cluster.name} ({total_mentions} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)")
    
    if idea_clusters:
        logger.info("üîç –¢–æ–ø-3 –∫–ª–∞—Å—Ç–µ—Ä–∞ –∏–¥–µ–π:")
        for i, cluster in enumerate(idea_clusters[:3]):
            total_mentions = sum(v.count for v in cluster.variants)
            logger.info(f"  {i+1}. {cluster.name} ({total_mentions} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π)")
    
    logger.info("‚úÖ Stage 4 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")


if __name__ == "__main__":
    main()
