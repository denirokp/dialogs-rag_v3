#!/usr/bin/env python3
"""
Stage 6: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤
–°–æ–∑–¥–∞–µ—Ç Markdown –∏ Excel –æ—Ç—á–µ—Ç—ã –≤ —Ç—Ä–µ–±—É–µ–º–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
"""

import json
import logging
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any

import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from config import settings
from models.validation import AggregateResults, Cluster, ClusterSlices
from prompts import STAGE_CONFIG

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def frac_of(total: int, part: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 'X –∏–∑ Y (Z%)'"""
    if total <= 0:
        return f"{part}"
    pct = 100.0 * part / total
    return f"{part} –∏–∑ {total} ({pct:.1f}%)"


def frac_pct_only(total: int, part: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 'X (Z%)'"""
    if total <= 0:
        return f"{part}"
    return f"{part} ({100.0 * part / total:.1f}%)"


def line_part_of(total: int, count: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 'X –∏–∑ Y (Z%)' —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –Ω–æ–ª—å"""
    return f"{count} –∏–∑ {total} ({(100.0*count/total):.1f}%)" if total else f"{count}"


def part_of(total: int, count: int) -> str:
    """–£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–µ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ 'X –∏–∑ Y (Z%)'"""
    return f"{count} –∏–∑ {total} ({(100.0*count/total):.1f}%)" if total else f"{count}"


def rare_badge(cluster: Dict[str, Any]) -> str:
    """–ë–µ–π–¥–∂ –¥–ª—è –µ–¥–∏–Ω–∏—á–Ω—ã—Ö –∫–µ–π—Å–æ–≤"""
    from config import settings
    return " *(–µ–¥–∏–Ω–∏—á–Ω—ã–π –∫–µ–π—Å)*" if cluster.get("mentions_abs", 0) < settings.rare_threshold else ""


def low_evidence_badge(cluster: Dict[str, Any]) -> str:
    """–ë–µ–π–¥–∂ –¥–ª—è –Ω–∏–∑–∫–æ–π –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º–æ—Å—Ç–∏"""
    low_evidence_share = cluster.get("low_evidence_share", 0)
    return " *(–Ω–∏–∑–∫–∞—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–∞–µ–º–æ—Å—Ç—å ‚Äî –º–∞–ª–æ —Ü–∏—Ç–∞—Ç)*" if low_evidence_share >= 0.5 else ""


def pick_cluster_quotes(quotes, max_per_cluster=3):
    """–í—ã–±–æ—Ä —Ü–∏—Ç–∞—Ç –∏–∑ —Ä–∞–∑–Ω—ã—Ö –¥–∏–∞–ª–æ–≥–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å –æ—á–∏—Å—Ç–∫–æ–π"""
    from pipeline.stage2_extract_entities import clean_sentence
    
    out, seen = [], set()
    for q in quotes:
        did = q.get("dialog_id")
        qt = q.get("quote", "").strip()
        if not qt or did in seen: 
            continue
        
        # –û—á–∏—â–∞–µ–º —Ü–∏—Ç–∞—Ç—É
        cleaned_quote = clean_sentence(qt)
        if len(cleaned_quote) < 10:  # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è –ø–æ—Å–ª–µ –æ—á–∏—Å—Ç–∫–∏
            continue
            
        seen.add(did)
        out.append({"dialog_id": did, "quote": cleaned_quote})
        if len(out) >= max_per_cluster: 
            break
    return out


def render_mentions(x, D):
    """–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π —Å –ø—Ä–æ—Ü–µ–Ω—Ç–∞–º–∏"""
    share = f"{(100*x/D):.1f}%" if D else "0%"
    return f"{x} –∏–∑ {D} ({share})"


def validate_cluster_data(cluster: Dict[str, Any], cluster_id: int) -> List[str]:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    errors = []
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è ID –∏ mentions_abs
    dialog_ids = cluster.get("dialog_ids", [])
    mentions_abs = cluster.get("mentions_abs", 0)
    
    if len(set(dialog_ids)) != mentions_abs:
        errors.append(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: IDs != mentions_abs ({len(set(dialog_ids))} != {mentions_abs})")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    sentiment = cluster.get("slices", {}).get("sentiment", {})
    if sentiment and mentions_abs > 0:
        total_sentiment = sum(sentiment.values())
        if total_sentiment > 0:
            total_pct = sum(100.0 * v / total_sentiment for v in sentiment.values())
            if not (99.0 <= total_pct <= 101.0):
                errors.append(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –Ω–µ ~100% ({total_pct:.1f}%)")
    
    return errors


def load_aggregate_results() -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏"""
    aggregate_file = Path("artifacts/aggregate_results.json")
    
    if not aggregate_file.exists():
        logger.error(f"‚ùå –§–∞–π–ª {aggregate_file} –Ω–µ –Ω–∞–π–¥–µ–Ω. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Stage 5 —Å–Ω–∞—á–∞–ª–∞.")
        return None
    
    with open(aggregate_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
        return data


def format_slices(slices: Dict[str, Any], D: int, mentions_abs: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å—Ä–µ–∑–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    lines = []
    
    if slices.get("regions"):
        lines.append("- –ü–æ —Ä–µ–≥–∏–æ–Ω–∞–º:")
        for region, count in sorted(slices["regions"].items(), key=lambda x: x[1], reverse=True):
            lines.append(f"  ‚Ä¢ {region}: {part_of(D, count)}")
    
    if slices.get("segments"):
        lines.append("- –ü–æ —Å–µ–≥–º–µ–Ω—Ç–∞–º:")
        for segment, count in sorted(slices["segments"].items(), key=lambda x: x[1], reverse=True):
            lines.append(f"  ‚Ä¢ {segment}: {part_of(D, count)}")
    
    if slices.get("product_categories"):
        lines.append("- –ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Ç–æ–≤–∞—Ä–æ–≤:")
        for category, count in sorted(slices["product_categories"].items(), key=lambda x: x[1], reverse=True):
            lines.append(f"  ‚Ä¢ {category}: {part_of(D, count)}")
    
    if slices.get("delivery_types"):
        lines.append("- –ü–æ —Ç–∏–ø–∞–º –¥–æ—Å—Ç–∞–≤–∫–∏:")
        for delivery_type, count in sorted(slices["delivery_types"].items(), key=lambda x: x[1], reverse=True):
            lines.append(f"  ‚Ä¢ {delivery_type}: {part_of(D, count)}")
    
    if slices.get("sentiment"):
        lines.append("- –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–≤–Ω—É—Ç—Ä–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞ = 100%):")
        for sentiment, count in sorted(slices["sentiment"].items(), key=lambda x: x[1], reverse=True):
            lines.append(f"  ‚Ä¢ {sentiment}: {part_of(mentions_abs, count)}")
    
    return "\n".join(lines)


def format_cluster_card(cluster: Dict[str, Any], cluster_id: int, D: int) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∫–∞—Ä—Ç–æ—á–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∞"""
    mentions_abs = cluster.get("mentions_abs", 0)
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    cluster_name = cluster.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
    rare_badge_text = rare_badge(cluster)
    low_evidence_badge_text = low_evidence_badge(cluster)
    lines = [f"### –ö–ª–∞—Å—Ç–µ—Ä {cluster_id}: {cluster_name}{rare_badge_text}{low_evidence_badge_text}"]
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    lines.append(f"- –£–ø–æ–º–∏–Ω–∞–Ω–∏–π –≤ —ç—Ç–æ–º –∫–ª–∞—Å—Ç–µ—Ä–µ: {part_of(D, mentions_abs)}")
    
    # –í–∞—Ä–∏–∞–Ω—Ç—ã —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫
    if cluster.get("variants"):
        lines.append("- –í–∞—Ä–∏–∞–Ω—Ç—ã —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–æ–∫:")
        for variant in cluster["variants"]:
            lines.append(f'  ‚Ä¢ "{variant["text"]}" ‚Äî {part_of(D, variant["count_abs"])}')
    
    # –°—Ä–µ–∑—ã –¥–∞–Ω–Ω—ã—Ö
    if cluster.get("slices"):
        slices_text = format_slices(cluster["slices"], D, mentions_abs)
        if slices_text:
            lines.append(slices_text)
    
    # ID –¥–∏–∞–ª–æ–≥–æ–≤
    if cluster.get("dialog_ids"):
        show_limit = STAGE_CONFIG["show_ids_limit"]
        ids = cluster["dialog_ids"]
        lines.append(f"- ID –¥–∏–∞–ª–æ–≥–æ–≤ ({mentions_abs} –∏–∑ {D}): {ids[:show_limit]}")
        if len(ids) > show_limit:
            lines.append(f"  *(–ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ ‚Äî –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ A)*")
    
    # –¶–∏—Ç–∞—Ç—ã –∫–ª–∏–µ–Ω—Ç–∞
    quotes = pick_cluster_quotes(cluster.get("quotes", []), max_per_cluster=3)
    if quotes:
        lines.append("- –¶–∏—Ç–∞—Ç—ã –∫–ª–∏–µ–Ω—Ç–∞ (–ø—Ä–∏–º–µ—Ä—ã):")
        for i, q in enumerate(quotes, 1):
            lines.append(f'  {i}) "{q["quote"]}" (Id: {q["dialog_id"]})')
    else:
        lines.append("- –¶–∏—Ç–∞—Ç—ã –∫–ª–∏–µ–Ω—Ç–∞: –Ω–µ –Ω–∞–π–¥–µ–Ω—ã (–≤–µ—Ä–æ—è—Ç–Ω–æ, –ø—Ä–æ–±–ª–µ–º–∞ –≤—ã—Ä–∞–∂–µ–Ω–∞ –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–º/–Ω–µ—è–≤–Ω–æ)")
    
    return "\n".join(lines)


def render_cluster_block(c, D, rare_threshold=3):
    """–†–µ–Ω–¥–µ—Ä–∏–Ω–≥ –±–ª–æ–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∞ —Å —É—á–µ—Ç–æ–º —Ä–µ–¥–∫–∏—Ö –∫–µ–π—Å–æ–≤"""
    rare = c["mentions_abs"] < rare_threshold
    # –ø—Ä–∏ rare ‚Äî –Ω–µ –ø–µ—á–∞—Ç–∞–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Å—Ä–µ–∑—ã
    return c, rare


def generate_markdown_report(results: Dict[str, Any]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –æ—Ç—á–µ—Ç–∞"""
    lines = []
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
    meta = results.get("meta", {})
    N = meta.get("N", 0)
    D = meta.get("D", 0)
    
    # –ó–∞–≥–æ–ª–æ–≤–æ–∫
    lines.extend([
        "# –û—Ç—á–µ—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–∏–∞–ª–æ–≥–æ–≤",
        "",
        f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*",
        "",
        "## –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ—Ö–≤–∞—Ç–∞ –ø–æ –¥–æ—Å—Ç–∞–≤–∫–µ",
        f"- –í—Å–µ–≥–æ –∑–≤–æ–Ω–∫–æ–≤ (N): {N}",
        f"- –° –¥–æ—Å—Ç–∞–≤–∫–æ–π (D): {part_of(N, D)}",
        f"- –ë–µ–∑ –¥–æ—Å—Ç–∞–≤–∫–∏: {N - D}",
        ""
    ])
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è —Ü–∏—Ç–∞—Ç –∫–ª–∏–µ–Ω—Ç–∞
    total_quotes = 0
    for category in ["barriers", "ideas", "signals"]:
        clusters = results.get(category, [])
        for cluster in clusters:
            quotes = cluster.get("quotes", [])
            total_quotes += len(quotes)
    
    if total_quotes == 0:
        lines.extend([
            "## ‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –¶–∏—Ç–∞—Ç—ã –∫–ª–∏–µ–Ω—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã",
            "",
            "–í —Ö–æ–¥–µ –∞–Ω–∞–ª–∏–∑–∞ –Ω–µ –±—ã–ª–æ –Ω–∞–π–¥–µ–Ω–æ –Ω–∏ –æ–¥–Ω–æ–π —Ü–∏—Ç–∞—Ç—ã –∫–ª–∏–µ–Ω—Ç–∞. –≠—Ç–æ –º–æ–∂–µ—Ç —É–∫–∞–∑—ã–≤–∞—Ç—å –Ω–∞:",
            "- –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ–º —Ü–∏—Ç–∞—Ç –∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤",
            "- –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö",
            "- –û—à–∏–±–∫–∏ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏",
            "",
            "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ü–∏—Ç–∞—Ç –≤ Stage 2.",
            ""
        ])
    
    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –æ –º–∞–ª–æ–π –≤—ã–±–æ—Ä–∫–µ
    warn_threshold = STAGE_CONFIG.get("small_sample_threshold", 50)
    if D < warn_threshold:
        lines.append(f"> ‚ö†Ô∏è –í–Ω–∏–º–∞–Ω–∏–µ: –º–∞–ª–∞—è –≤—ã–±–æ—Ä–∫–∞. D < {warn_threshold}, –ø—Ä–æ—Ü–µ–Ω—Ç—ã –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω—ã–µ.\n")
    
    # –ë–∞—Ä—å–µ—Ä—ã
    if results.get("barriers"):
        lines.extend([
            "## –ë–∞—Ä—å–µ—Ä—ã",
            ""
        ])
        
        for i, cluster in enumerate(results["barriers"], 1):
            cluster_card = format_cluster_card(cluster, i, D)
            lines.append(cluster_card)
            lines.append("")
    
    # –ò–¥–µ–∏
    if results.get("ideas"):
        lines.extend([
            "## –ò–¥–µ–∏",
            ""
        ])
        
        for i, cluster in enumerate(results["ideas"], 1):
            cluster_card = format_cluster_card(cluster, i, D)
            lines.append(cluster_card)
            lines.append("")
    
    # –°–∏–≥–Ω–∞–ª—ã –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
    if results.get("signals_platform"):
        lines.extend([
            "## –°–∏–≥–Ω–∞–ª—ã –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã",
            ""
        ])
        
        for i, cluster in enumerate(results["signals_platform"], 1):
            cluster_card = format_cluster_card(cluster, i, D)
            lines.append(cluster_card)
            lines.append("")
    
    # –°–∏–≥–Ω–∞–ª—ã –¥–æ—Å—Ç–∞–≤–∫–∏
    if results.get("signals"):
        lines.extend([
            "## –°–∏–≥–Ω–∞–ª—ã –¥–æ—Å—Ç–∞–≤–∫–∏",
            ""
        ])
        
        for i, cluster in enumerate(results["signals"], 1):
            cluster_card = format_cluster_card(cluster, i, D)
            lines.append(cluster_card)
            lines.append("")
    
    return "\n".join(lines)


def generate_appendix_ids(results: Dict[str, Any]) -> str:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –ø–æ–ª–Ω—ã–º–∏ —Å–ø–∏—Å–∫–∞–º–∏ ID"""
    lines = []
    
    lines.extend([
        "# –ü—Ä–∏–ª–æ–∂–µ–Ω–∏–µ A: –ü–æ–ª–Ω—ã–µ —Å–ø–∏—Å–∫–∏ ID –¥–∏–∞–ª–æ–≥–æ–≤",
        "",
        f"*–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}*",
        ""
    ])
    
    # –ë–∞—Ä—å–µ—Ä—ã
    if results.get("barriers"):
        lines.extend([
            "## –ë–∞—Ä—å–µ—Ä—ã",
            ""
        ])
        
        for i, cluster in enumerate(results["barriers"], 1):
            lines.extend([
                f"### –ö–ª–∞—Å—Ç–µ—Ä {i}: {cluster.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}",
                f"ID –¥–∏–∞–ª–æ–≥–æ–≤ ({len(cluster.get('dialog_ids', []))}):",
                ", ".join(cluster.get('dialog_ids', [])),
                ""
            ])
    
    # –ò–¥–µ–∏
    if results.get("ideas"):
        lines.extend([
            "## –ò–¥–µ–∏",
            ""
        ])
        
        for i, cluster in enumerate(results["ideas"], 1):
            lines.extend([
                f"### –ö–ª–∞—Å—Ç–µ—Ä {i}: {cluster.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}",
                f"ID –¥–∏–∞–ª–æ–≥–æ–≤ ({len(cluster.get('dialog_ids', []))}):",
                ", ".join(cluster.get('dialog_ids', [])),
                ""
            ])
    
    # –°–∏–≥–Ω–∞–ª—ã –¥–æ—Å—Ç–∞–≤–∫–∏
    if results.get("signals"):
        lines.extend([
            "## –°–∏–≥–Ω–∞–ª—ã –¥–æ—Å—Ç–∞–≤–∫–∏",
            ""
        ])
        
        for i, cluster in enumerate(results["signals"], 1):
            lines.extend([
                f"### –ö–ª–∞—Å—Ç–µ—Ä {i}: {cluster.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}",
                f"ID –¥–∏–∞–ª–æ–≥–æ–≤ ({len(cluster.get('dialog_ids', []))}):",
                ", ".join(cluster.get('dialog_ids', [])),
                ""
            ])
    
    # –°–∏–≥–Ω–∞–ª—ã –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã
    if results.get("signals_platform"):
        lines.extend([
            "## –°–∏–≥–Ω–∞–ª—ã –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã",
            ""
        ])
        
        for i, cluster in enumerate(results["signals_platform"], 1):
            lines.extend([
                f"### –ö–ª–∞—Å—Ç–µ—Ä {i}: {cluster.get('name', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')}",
                f"ID –¥–∏–∞–ª–æ–≥–æ–≤ ({len(cluster.get('dialog_ids', []))}):",
                ", ".join(cluster.get('dialog_ids', [])),
                ""
            ])
    
    return "\n".join(lines)


def generate_excel_report(results: Dict[str, Any]):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è Excel –æ—Ç—á–µ—Ç–∞"""
    reports_dir = Path(settings.output_dir)
    reports_dir.mkdir(exist_ok=True)
    
    excel_file = reports_dir / "report.xlsx"
    
    with pd.ExcelWriter(excel_file, engine='openpyxl') as writer:
        sheets_created = False
        
        # –ë–∞—Ä—å–µ—Ä—ã
        if results.get("barriers"):
            barrier_data = []
            for i, cluster in enumerate(results["barriers"], 1):
                total_mentions = cluster.get("mentions_abs", 0)
                percentage = cluster.get("mentions_pct_of_D", 0)
                
                barrier_data.append({
                    "cluster_id": i,
                    "name": cluster.get("name", ""),
                    "total_mentions": total_mentions,
                    "percentage": percentage,
                    "unique_dialogs": len(cluster.get("dialog_ids", [])),
                    "variants_json": json.dumps(cluster.get("variants", []), ensure_ascii=False),
                    "slices_json": json.dumps(cluster.get("slices", {}), ensure_ascii=False),
                    "dialog_ids_json": json.dumps(cluster.get("dialog_ids", []), ensure_ascii=False)
                })
            
            barrier_df = pd.DataFrame(barrier_data)
            barrier_df.to_excel(writer, sheet_name='barriers', index=False)
            sheets_created = True
        
        # –ò–¥–µ–∏
        if results.get("ideas"):
            idea_data = []
            for i, cluster in enumerate(results["ideas"], 1):
                total_mentions = cluster.get("mentions_abs", 0)
                percentage = cluster.get("mentions_pct_of_D", 0)
                
                idea_data.append({
                    "cluster_id": i,
                    "name": cluster.get("name", ""),
                    "total_mentions": total_mentions,
                    "percentage": percentage,
                    "unique_dialogs": len(cluster.get("dialog_ids", [])),
                    "variants_json": json.dumps(cluster.get("variants", []), ensure_ascii=False),
                    "slices_json": json.dumps(cluster.get("slices", {}), ensure_ascii=False),
                    "dialog_ids_json": json.dumps(cluster.get("dialog_ids", []), ensure_ascii=False)
                })
            
            idea_df = pd.DataFrame(idea_data)
            idea_df.to_excel(writer, sheet_name='ideas', index=False)
            sheets_created = True
        
        # –°–∏–≥–Ω–∞–ª—ã
        if results.get("signals"):
            signal_data = []
            for i, cluster in enumerate(results["signals"], 1):
                total_mentions = cluster.get("mentions_abs", 0)
                percentage = cluster.get("mentions_pct_of_D", 0)
                
                signal_data.append({
                    "cluster_id": i,
                    "name": cluster.get("name", ""),
                    "total_mentions": total_mentions,
                    "percentage": percentage,
                    "unique_dialogs": len(cluster.get("dialog_ids", [])),
                    "variants_json": json.dumps(cluster.get("variants", []), ensure_ascii=False),
                    "slices_json": json.dumps(cluster.get("slices", {}), ensure_ascii=False),
                    "dialog_ids_json": json.dumps(cluster.get("dialog_ids", []), ensure_ascii=False)
                })
            
            signal_df = pd.DataFrame(signal_data)
            signal_df.to_excel(writer, sheet_name='signals', index=False)
            sheets_created = True
        
        # –ï—Å–ª–∏ –Ω–µ —Å–æ–∑–¥–∞–Ω–æ –Ω–∏ –æ–¥–Ω–æ–≥–æ –ª–∏—Å—Ç–∞, —Å–æ–∑–¥–∞–µ–º –ø—É—Å—Ç–æ–π
        if not sheets_created:
            empty_df = pd.DataFrame({'message': ['–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è']})
            empty_df.to_excel(writer, sheet_name='summary', index=False)
    
    return excel_file


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è Stage 6"""
    logger.info("üöÄ Stage 6: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á–µ—Ç–æ–≤")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–ø–∫–∏ reports
    reports_dir = Path(settings.output_dir)
    reports_dir.mkdir(exist_ok=True)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–≥—Ä–µ–≥–∞—Ü–∏–∏
    results = load_aggregate_results()
    if not results:
        return
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –æ—Ç—á–µ—Ç–∞
    logger.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Markdown –æ—Ç—á–µ—Ç–∞...")
    markdown_content = generate_markdown_report(results)
    
    markdown_file = reports_dir / "report.md"
    with open(markdown_file, 'w', encoding='utf-8') as f:
        f.write(markdown_content)
    
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω Markdown –æ—Ç—á–µ—Ç: {markdown_file}")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å ID
    logger.info("üìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å ID...")
    appendix_content = generate_appendix_ids(results)
    
    appendix_file = reports_dir / "appendix_ids.md"
    with open(appendix_file, 'w', encoding='utf-8') as f:
        f.write(appendix_content)
    
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ —Å ID: {appendix_file}")
    
    # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Excel –æ—Ç—á–µ—Ç–∞
    logger.info("üìä –ì–µ–Ω–µ—Ä–∞—Ü–∏—è Excel –æ—Ç—á–µ—Ç–∞...")
    excel_file = generate_excel_report(results)
    
    logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω Excel –æ—Ç—á–µ—Ç: {excel_file}")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –¥–∞–Ω–Ω—ã—Ö...")
    all_errors = []
    
    for category in ["barriers", "ideas", "signals"]:
        clusters = results.get(category, [])
        for i, cluster in enumerate(clusters, 1):
            errors = validate_cluster_data(cluster, i)
            all_errors.extend(errors)
    
    if all_errors:
        logger.warning("‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã –æ—à–∏–±–∫–∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏:")
        for error in all_errors:
            logger.warning(f"  {error}")
    else:
        logger.info("‚úÖ –í—Å–µ –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ –ø—Ä–æ–π–¥–µ–Ω—ã")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    meta = results.get("meta", {})
    N = meta.get("N", 0)
    D = meta.get("D", 0)
    
    logger.info("üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç—á–µ—Ç–æ–≤:")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –±–∞—Ä—å–µ—Ä–æ–≤: {len(results.get('barriers', []))}")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ –∏–¥–µ–π: {len(results.get('ideas', []))}")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–æ—Å—Ç–∞–≤–∫–∏: {len(results.get('signals', []))}")
    logger.info(f"  –ö–ª–∞—Å—Ç–µ—Ä–æ–≤ —Å–∏–≥–Ω–∞–ª–æ–≤ –ø–ª–∞—Ç—Ñ–æ—Ä–º—ã: {len(results.get('signals_platform', []))}")
    logger.info(f"  –í—Å–µ–≥–æ –¥–∏–∞–ª–æ–≥–æ–≤ (N): {N}")
    logger.info(f"  –° –¥–æ—Å—Ç–∞–≤–∫–æ–π (D): {D} –∏–∑ {N} ({100.0*D/N:.1f}%)")
    
    logger.info("‚úÖ Stage 6 –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
    logger.info(f"üìÅ –û—Ç—á–µ—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –ø–∞–ø–∫–µ: {reports_dir}")


if __name__ == "__main__":
    main()
