# -*- coding: utf-8 -*-
"""
LLM-based –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä (–∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ —Ç–µ–∫—É—â—É—é —Å–∏—Å—Ç–µ–º—É):
- –ß–∏—Ç–∞–µ—Ç Excel: data/input/dialogs14_09.xlsx —Å –∫–æ–ª–æ–Ω–∫–∞–º–∏ ["ID –∑–≤–æ–Ω–∫–∞","–¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏"]
- –†–µ–∂–µ—Ç –¥–∏–∞–ª–æ–≥–∏ –Ω–∞ —Ä–µ–ø–ª–∏–∫–∏, –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ client-only
- –ì–∏–±—Ä–∏–¥ whole-dialog / –æ–∫–Ω–∞
- LLM –∏–∑–≤–ª–µ–∫–∞–µ—Ç —É–ø–æ–º–∏–Ω–∞–Ω–∏—è (problems/ideas/signals) –ø–æ taxonomy.yaml
- –î–µ–¥—É–ø –∏ –∑–∞–ø–∏—Å—å –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤:
  - artifacts/comprehensive_results.json   ({"mentions": [...]})
  - artifacts/statistics.json              (–ø–µ—Ä–µ—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è)
–¢—Ä–µ–±—É–µ—Ç—Å—è: OPENAI_API_KEY; pip install -r requirements.txt
"""

import os, re, json, math, hashlib, argparse, time
import httpx, pandas as pd, yaml
from pathlib import Path
from typing import List, Dict, Any
from tqdm import tqdm

INPUT_XLSX = "data/input/dialogs15_09(2000).xlsx"
ART_DIR = Path("artifacts")
ART_DIR.mkdir(parents=True, exist_ok=True)
RES_PATH = ART_DIR / "comprehensive_results.json"
STATS_PATH = ART_DIR / "statistics.json"
TAX_PATH = "taxonomy.yaml"

# ----------------- —á—Ç–µ–Ω–∏–µ, –ø–∞—Ä—Å–∏–Ω–≥, client-only -----------------
def read_dialogs(path: str) -> pd.DataFrame:
    df = pd.read_excel(path)
    df = df.rename(columns={"ID –∑–≤–æ–Ω–∫–∞":"dialog_id","–¢–µ–∫—Å—Ç —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏":"full_text"})
    assert {"dialog_id","full_text"} <= set(df.columns)
    df["dialog_id"] = df["dialog_id"].astype(str)
    return df[["dialog_id","full_text"]]

# –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤ —Ä–æ–ª–µ–π –∏ —Ä–∞–∑–¥–µ–ª–∏—Ç–µ–ª–µ–π (":" –∏–ª–∏ "-")
ROLE_PATTERNS = {
    "client": re.compile(r"^(?:–ö–ª–∏–µ–Ω—Ç|–ü–æ–∫—É–ø–∞—Ç–µ–ª—å)\s*[:\-]\s*(.*)$", re.IGNORECASE),
    "operator": re.compile(r"^(?:–û–ø–µ—Ä–∞—Ç–æ—Ä|–ú–µ–Ω–µ–¥–∂–µ—Ä)\s*[:\-]\s*(.*)$", re.IGNORECASE),
}

def split_turns(full_text: str) -> List[Dict[str,Any]]:
    turns = []
    tid = 0
    for raw in (full_text or "").splitlines():
        raw = str(raw).strip()
        if not raw:
            continue
        m_client = ROLE_PATTERNS["client"].match(raw)
        m_oper = ROLE_PATTERNS["operator"].match(raw)
        if m_client:
            role = "client"; text = m_client.group(1).strip()
        elif m_oper:
            role = "operator"; text = m_oper.group(1).strip()
        else:
            continue
        tid += 1
        turns.append({"turn_id": tid, "role": role, "text": text})
    return turns

def client_only_windows(turns: List[Dict[str,Any]], whole_max_tokens=8000, window_tokens=1800):
    # –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ ~ —Å–∏–º–≤–æ–ª—ã/4
    client_turns = [t for t in turns if t["role"] == "client"]
    total_chars = sum(len(t["text"]) for t in client_turns)
    if math.ceil(total_chars/4) <= whole_max_tokens:
        return [{"mode":"whole","window_id":0,"turns":client_turns}]
    # –æ–∫–Ω–∞
    out, acc, acc_len, widx = [], [], 0, 0
    max_chars = window_tokens*4
    for t in client_turns:
        L = len(t["text"])
        if acc and acc_len + L > max_chars:
            out.append({"mode":"window","window_id":widx,"turns":acc})
            widx, acc, acc_len = widx+1, [], 0
        acc.append(t); acc_len += L
    if acc: out.append({"mode":"window","window_id":widx,"turns":acc})
    return out

def format_for_prompt(window) -> str:
    return "\n".join([f"[{t['turn_id']}] {t['text']}" for t in window["turns"]])

# ----------------- LLM —ç–∫—Å—Ç—Ä–∞–∫—Ü–∏—è -----------------
SYSTEM = (
    "–¢—ã –∏–∑–≤–ª–µ–∫–∞–µ—à—å —Ç–æ–ª—å–∫–æ –∏–∑ —Å–ª–æ–≤ –ö–õ–ò–ï–ù–¢–ê. –í–µ—Ä–Ω–∏ –û–î–ò–ù JSON-–û–ë–™–ï–ö–¢ –≤–∏–¥–∞ "
    '{"mentions":[{...}]}. –î–ª—è –∫–∞–∂–¥–æ–≥–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∫–ª—é—á–∏: '
    "dialog_id, turn_id, label_type (problems|ideas|signals), "
    "theme, subtheme, text_quote, confidence (0..1). –ë–µ–∑ –ø–æ—è—Å–Ω–µ–Ω–∏–π."
)

USER_TMPL = (
    "–¢–∞–∫—Å–æ–Ω–æ–º–∏—è (themes‚Üísubthemes):\n{taxonomy}\n---\n"
    "–û–∫–Ω–æ –¥–∏–∞–ª–æ–≥–∞ (—Ç–æ–ª—å–∫–æ –∫–ª–∏–µ–Ω—Ç):\n{window}\n---\n"
    "–í–µ—Ä–Ω–∏ JSON-–æ–±—ä–µ–∫—Ç {{\"mentions\":[...]}} —Å–æ —Å—Ç—Ä–æ–≥–∏–º–∏ –∫–ª—é—á–∞–º–∏."
)

class LLM:
    def __init__(self, model="gpt-4o-mini", timeout=120):
        self.model = model
        self.key = os.getenv("OPENAI_API_KEY", "")
        self.client = httpx.Client(timeout=timeout)

    def extract(self, dialog_id: str, window) -> List[Dict[str,Any]]:
        if not self.key:
            raise RuntimeError("ENV OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω")
        with open(TAX_PATH, "r", encoding="utf-8") as f:
            taxonomy = yaml.safe_load(f)
        user = USER_TMPL.format(
            taxonomy=json.dumps(taxonomy, ensure_ascii=False),
            window=format_for_prompt(window),
        )
        payload = {
            "model": self.model,
            "temperature": 0,
            "response_format": {"type": "json_object"},
            "messages": [
                {"role": "system", "content": SYSTEM},
                {"role": "user", "content": user},
            ],
        }
        r = self.client.post(
            "https://api.openai.com/v1/chat/completions",
            headers={
                "Authorization": f"Bearer {self.key}",
                "Content-Type": "application/json",
            },
            json=payload,
        )
        r.raise_for_status()
        content = r.json()["choices"][0]["message"]["content"]
        try:
            js = json.loads(content)
            arr = js.get("mentions", [])
        except Exception:
            arr = []
        out = []
        for m in arr:
            if not isinstance(m, dict):
                continue
            text_quote = (m.get("text_quote") or "").strip()
            if not text_quote:
                continue
            turn_id = m.get("turn_id")
            try:
                turn_id = int(turn_id)
            except:
                mt = re.search(r"\[(\d+)\]", text_quote)
                turn_id = int(mt.group(1)) if mt else 0
            out.append({
                "dialog_id": dialog_id,
                "turn_id": turn_id,
                "label_type": (m.get("label_type") or "problems"),
                "theme": (m.get("theme") or "").strip(),
                "subtheme": (m.get("subtheme") or "").strip(),
                "text_quote": text_quote,
                "confidence": float(m.get("confidence") or 0.5),
            })
        return out

# ----------------- –¥–µ–¥—É–ø -----------------
def norm_quote(s: str) -> str:
    return re.sub(r"\s+", " ", s.strip().lower())

def dedup_mentions(rows: List[Dict[str,Any]]) -> List[Dict[str,Any]]:
    seen = set(); out = []
    for r in rows:
        key = (
            r["dialog_id"],
            r["turn_id"],
            r["label_type"],
            r["theme"],
            r.get("subtheme"),
            hashlib.md5(norm_quote(r["text_quote"]).encode()).hexdigest(),
        )
        if key in seen:
            continue
        seen.add(key); out.append(r)
    return out

# ----------------- –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–≥–æ–Ω -----------------
def run(model="gpt-4o-mini", whole_max=8000, window_tokens=1800):
    df = read_dialogs(INPUT_XLSX)
    llm = LLM(model=model)
    all_mentions: List[Dict[str,Any]] = []
    
    total_dialogs = len(df)
    print(f"üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∞–Ω–∞–ª–∏–∑ {total_dialogs} –¥–∏–∞–ª–æ–≥–æ–≤...")
    print(f"üìä –ú–æ–¥–µ–ª—å: {model}, –æ–∫–Ω–æ: {window_tokens} —Ç–æ–∫–µ–Ω–æ–≤")
    
    start_time = time.time()
    
    # –ü—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –¥–ª—è –¥–∏–∞–ª–æ–≥–æ–≤
    with tqdm(total=total_dialogs, desc="üìû –ê–Ω–∞–ª–∏–∑ –¥–∏–∞–ª–æ–≥–æ–≤", unit="–¥–∏–∞–ª–æ–≥") as pbar:
        for idx, row in df.iterrows():
            dlg_id = row["dialog_id"]
            turns = split_turns(row["full_text"])
            windows = client_only_windows(
                turns, whole_max_tokens=whole_max, window_tokens=window_tokens
            )
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –æ–∫–Ω–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞
            for w in windows:
                all_mentions.extend(llm.extract(dlg_id, w))
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å
            pbar.update(1)
            
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∫–∞–∂–¥—ã–µ 50 –¥–∏–∞–ª–æ–≥–æ–≤
            if (idx + 1) % 50 == 0:
                elapsed = time.time() - start_time
                rate = (idx + 1) / elapsed
                eta = (total_dialogs - idx - 1) / rate if rate > 0 else 0
                
                pbar.set_postfix({
                    '–Ω–∞–π–¥–µ–Ω–æ': len(all_mentions),
                    '—Å–∫–æ—Ä–æ—Å—Ç—å': f'{rate:.1f} –¥–∏–∞–ª/–º–∏–Ω',
                    '–æ—Å—Ç–∞–ª–æ—Å—å': f'{eta/60:.1f} –º–∏–Ω'
                })

    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    total_time = time.time() - start_time
    print(f"\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {total_time/60:.1f} –º–∏–Ω—É—Ç")
    print(f"üìä –°–∫–æ—Ä–æ—Å—Ç—å: {total_dialogs/(total_time/60):.1f} –¥–∏–∞–ª–æ–≥–æ–≤/–º–∏–Ω—É—Ç—É")
    
    # –î–æ –¥–µ–¥—É–ø–∞
    pre_count = len(all_mentions)
    print(f"üîç –ù–∞–π–¥–µ–Ω–æ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π: {pre_count}")
    
    all_mentions = dedup_mentions(all_mentions)
    dedup_removed_pct = round(100 * (1 - len(all_mentions) / max(1, pre_count)), 1)
    ambiguity_pct = round(
        100 * sum(1 for m in all_mentions if m.get("confidence", 0) < 0.6) / max(1, len(all_mentions)),
        1,
    )
    
    print(f"üßπ –ü–æ—Å–ª–µ –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–∏: {len(all_mentions)} —É–ø–æ–º–∏–Ω–∞–Ω–∏–π")
    print(f"üìà –£–¥–∞–ª–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {dedup_removed_pct}%")
    print(f"‚ö†Ô∏è  –ù–∏–∑–∫–æ—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö: {ambiguity_pct}%")

    # –∑–∞–ø–∏—à–µ–º –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã
    RES_PATH.write_text(
        json.dumps({"mentions": all_mentions}, ensure_ascii=False, indent=2),
        encoding="utf-8",
    )

    # –ø–µ—Ä–µ—Å—á—ë—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏
    stats = {
        "dialogs": int(df["dialog_id"].nunique()),
        "mentions": len(all_mentions),
        "problems": sum(1 for m in all_mentions if m["label_type"] == "problems"),
        "ideas": sum(1 for m in all_mentions if m["label_type"] == "ideas"),
        "signals": sum(1 for m in all_mentions if m["label_type"] == "signals"),
        "evidence_100": (len(all_mentions) > 0 and all(m.get("text_quote") for m in all_mentions)),
        "dedup_removed_pct": dedup_removed_pct,
        "ambiguity_pct": ambiguity_pct,
    }
    STATS_PATH.write_text(json.dumps(stats, ensure_ascii=False, indent=2), encoding="utf-8")
    print(f"[ok] mentions={len(all_mentions)}  dialogs={stats['dialogs']}  saved -> {RES_PATH}")

if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--model", default="gpt-4o-mini")
    ap.add_argument("--whole_max", type=int, default=8000)
    ap.add_argument("--window_tokens", type=int, default=1800)
    args = ap.parse_args()
    run(model=args.model, whole_max=args.whole_max, window_tokens=args.window_tokens)